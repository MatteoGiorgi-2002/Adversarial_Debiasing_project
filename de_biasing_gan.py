# -*- coding: utf-8 -*-
"""De-biasing-GAN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/matteogiorgi000/de-biasing-gan.5a5a9574-50e3-4df3-b256-e473c86e2951.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250628/auto/storage/goog4_request%26X-Goog-Date%3D20250628T103433Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5d2c7d15ceee1ba519c17696230b419339c329951c1089e445b7f705f2827a813ae645df378b6e697be7790de7729953f58ef961df7f63dcec9e0539ddcdea5b01439a867870f521254317aa04d34be4bf7c16c6e4ed6e11ea89abf5075e73ddde40feabf7beac02ff0023182845a3d8c342dd121d2cc2cba49bd5b3797f97497cdaa205262ff4facc7b09a361a6ff4b399e2929d09288e97975a49b259058156ca936a1cd1c178fad5fa44fb161c4f7b86ed2a45f7b5424df4a41c4682218d8a90acca3ae072452ef4e31959187c8296b2e9dd42c0fcad0160e9e47864782749a749ca21edd893a33ca29911a99b17c6fcc2ca2f13095e3849d3174216ab8de
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

matteogiorgi000_prof_dataset_path = kagglehub.dataset_download('matteogiorgi000/prof-dataset')
matteogiorgi000_cv_dataset_facebook_path = kagglehub.dataset_download('matteogiorgi000/cv-dataset-facebook')
matteogiorgi000_cv_dataset_done1_path = kagglehub.dataset_download('matteogiorgi000/cv-dataset-done1')
matteogiorgi000_cvs_keggle_path = kagglehub.dataset_download('matteogiorgi000/cvs-keggle')
matteogiorgi000_processed_prof_dataset_translated_path = kagglehub.dataset_download('matteogiorgi000/processed-prof-dataset-translated')

print('Data source import complete.')

!pip install python-docx

!pip install PyPDF2

!pip install --upgrade pip
#!pip install tensorflow==2.15.0 -q
!pip install --upgrade transformers==4.52.1 accelerate

"""# PREPROCESSING OF THE TWO DATASETS:
- use all-MiniLM-L6-v2 to generate semantic embeddings (not used in my later cells)
-  full text extraction from PDF using PyPDF2
"""

# Fix the IPython widgets issue first
!pip install --upgrade ipywidgets
!pip install --upgrade notebook

import pandas as pd
import numpy as np
# Import regular tqdm instead of notebook version to avoid widget issues
from tqdm import tqdm
import pickle
try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    !pip install sentence-transformers
    from sentence_transformers import SentenceTransformer

# Function to generate embeddings
def generate_embeddings(texts, ids, model_name='all-MiniLM-L6-v2'):
    print(f"Generating embeddings using {model_name}...")
    model = SentenceTransformer(model_name)
    embeddings = []

    # Use regular tqdm instead of tqdm.notebook to avoid widget issues
    for i, text in enumerate(tqdm(texts, desc="Generating embeddings")):
        # Ensure text is a string and not empty
        if not isinstance(text, str):
            print(f"Warning: Item {i} is not a string. Converting to string.")
            text = str(text) if text is not None else ""

        # Clean and prepare text for better embedding quality
        if text and len(text.strip()) > 0:
            try:
                embedding = model.encode(text)
                embeddings.append(embedding)
            except Exception as e:
                print(f"Error encoding text at index {i}: {e}")
                # Use zero vector if encoding fails
                embeddings.append(np.zeros(model.get_sentence_embedding_dimension()))
        else:
            # Use zero vector for empty text
            print(f"Warning: Empty text at index {i}. Using zero vector.")
            embeddings.append(np.zeros(model.get_sentence_embedding_dimension()))

    # Create a dictionary mapping ID to embedding
    embedding_dict = {ids[i]: embeddings[i] for i in range(len(ids))}

    return embeddings, embedding_dict

# Main function to process the translated dataset
def process_english_text_dataset(csv_path):
    print(f"Loading data from {csv_path}...")
    # Read the CSV file
    df = pd.read_csv(csv_path)

    print(f"Dataset shape: {df.shape}")
    print(f"Columns: {df.columns.tolist()}")

    # Ensure the required columns exist
    if 'id' not in df.columns or 'full_text_english' not in df.columns:
        missing_cols = []
        if 'id' not in df.columns: missing_cols.append('id')
        if 'full_text_english' not in df.columns: missing_cols.append('full_text_english')
        raise ValueError(f"Missing required columns: {', '.join(missing_cols)}")

    # Clean the data
    # Fill any missing values in full_text_english with empty strings
    df['full_text_english'] = df['full_text_english'].fillna("")

    # Convert any non-string values to strings
    for i, val in enumerate(df['full_text_english']):
        if not isinstance(val, str):
            df.loc[i, 'full_text_english'] = str(val) if val is not None else ""

    # Prepare data for embedding
    texts = df["full_text_english"].tolist()
    ids = df["id"].tolist()

    # Generate embeddings
    embeddings, embedding_dict = generate_embeddings(texts, ids)

    # Save embeddings to files
    with open("cv_english_embeddings.pkl", "wb") as f:
        pickle.dump(embeddings, f)

    # Save embeddings with IDs to file
    with open("cv_english_embeddings_with_ids.pkl", "wb") as f:
        pickle.dump(embedding_dict, f)

    print(f"Generated and saved embeddings for {len(embeddings)} documents")
    print(f"Saved ID-to-embedding mapping for later training use")

    # Display embedding information
    if embeddings and len(embeddings) > 0:
        print(f"\nEmbedding shape: {embeddings[0].shape}")
        print(f"Embedding sample (first 5 values): {embeddings[0][:5]}")
        print(f"Number of IDs in embedding dictionary: {len(embedding_dict)}")
        if embedding_dict:
            sample_id = list(embedding_dict.keys())[0]
            print(f"Sample ID in embedding dictionary: {sample_id}")

    return df, embeddings, embedding_dict

# Make sure we have the necessary packages
!pip install sentence-transformers

# Path to the CSV file
csv_path = "/kaggle/input/processed-prof-dataset-translated/processed_prof_dataset_ (5)_translated (3).csv"

# Process the dataset
df, embeddings, embedding_dict = process_english_text_dataset(csv_path)

# Show overall statistics
print("\nOverall Dataset Statistics:")
print(f"Total documents processed: {len(df)}")
print(f"Average text length: {df['full_text_english'].str.len().mean():.0f} characters")
print(f"Minimum text length: {df['full_text_english'].str.len().min()} characters")
print(f"Maximum text length: {df['full_text_english'].str.len().max()} characters")

"""### REPEATING FULL TEXT EXTRACTION AND EMBEDDING + 24 LABELS FOR KEGGLE DATASET"""

import os
import pandas as pd
import re
import numpy as np
# Import regular tqdm instead of notebook version to avoid widget issues
from tqdm import tqdm
import pickle
try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    !pip install sentence-transformers
    from sentence_transformers import SentenceTransformer

# Install necessary packages if not already installed
try:
    import fitz  # PyMuPDF
except ImportError:
    !pip install pymupdf
    import fitz

# Function to extract ID from CV filename
def extract_id(filename):
    # Extract just the numeric ID from the filename
    match = re.search(r'(\d+)', filename)
    if match:
        return match.group(1)
    return None

# Function to extract text from PDF using PyMuPDF (fitz)
def extract_text_from_pdf(pdf_path):
    text = ""
    try:
        # Open the PDF with PyMuPDF
        doc = fitz.open(pdf_path)

        # Extract text from each page
        for page_num in range(len(doc)):
            page = doc[page_num]

            # Get text with more preservation of layout
            text += page.get_text("text") + "\n\n"

        # Close the document
        doc.close()

        return text  # Will be a string, even if empty
    except Exception as e:
        print(f"Error extracting text from {pdf_path}: {e}")
        return ""  # Return empty string instead of None or NaN

# Function to find all CV files in the dataset with job categories
def find_cv_files(base_dir):
    cv_files = []

    print(f"Scanning directory: {base_dir}")

    # List all job category directories
    try:
        job_categories = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]
        print(f"Found {len(job_categories)} job categories: {', '.join(job_categories)}")

        # Walk through each job category directory
        for job_category in job_categories:
            job_dir = os.path.join(base_dir, job_category)

            for root, dirs, files in os.walk(job_dir):
                for file in files:
                    if file.lower().endswith('.pdf'):
                        id = extract_id(file)
                        if id:
                            cv_files.append({
                                'id': id,
                                'job_category': job_category,
                                'path': os.path.join(root, file)
                            })
    except Exception as e:
        print(f"Error scanning directories: {e}")

    print(f"Found {len(cv_files)} CV files across all job categories")

    return cv_files

# Function to generate embeddings
def generate_embeddings(texts, ids, model_name='all-MiniLM-L6-v2'):
    print(f"Generating embeddings using {model_name}...")
    model = SentenceTransformer(model_name)
    embeddings = []

    # Use regular tqdm instead of tqdm.notebook to avoid widget issues
    for i, text in enumerate(tqdm(texts, desc="Generating embeddings")):
        # Ensure text is a string and not empty
        if not isinstance(text, str):
            print(f"Warning: Item {i} is not a string. Converting to string.")
            text = str(text) if text is not None else ""

        # Clean and prepare text for better embedding quality
        if text and len(text.strip()) > 0:
            try:
                embedding = model.encode(text)
                embeddings.append(embedding)
            except Exception as e:
                print(f"Error encoding text at index {i}: {e}")
                # Use zero vector if encoding fails
                embeddings.append(np.zeros(model.get_sentence_embedding_dimension()))
        else:
            # Use zero vector for empty text
            print(f"Warning: Empty text at index {i}. Using zero vector.")
            embeddings.append(np.zeros(model.get_sentence_embedding_dimension()))

    # Create a dictionary mapping ID to embedding
    embedding_dict = {ids[i]: embeddings[i] for i in range(len(ids))}

    return embeddings, embedding_dict

# Main function to process the dataset
def process_cv_dataset(base_dir, generate_embedding=True):
    # Find all CV files with their job categories
    cv_files = find_cv_files(base_dir)

    # Create a list to store processed data
    processed_data = []

    # Use regular tqdm to avoid widget issues
    # Process CV files
    for cv_file in tqdm(cv_files, desc="Processing CV files"):
        cv_id = cv_file['id']
        job_category = cv_file['job_category']
        cv_path = cv_file['path']

        # Extract text from PDF
        cv_text = extract_text_from_pdf(cv_path)

        # Create a record
        record = {
            "id": cv_id,
            "job_category": job_category,
            "full_text": cv_text if cv_text else ""  # Ensure we have a string, not None
        }

        processed_data.append(record)

    print(f"Successfully processed {len(processed_data)} CV records")

    # Convert to DataFrame for easier analysis
    df = pd.DataFrame(processed_data)

    # Clean the DataFrame - replace any NaN values with empty strings
    if 'full_text' in df.columns:
        df['full_text'] = df['full_text'].fillna("")
        # Check for non-string values and convert them
        for i, val in enumerate(df['full_text']):
            if not isinstance(val, str):
                df.loc[i, 'full_text'] = str(val) if val is not None else ""

    # Generate embeddings if requested
    embeddings = None
    embedding_dict = None
    if generate_embedding and len(df) > 0:
        texts = df["full_text"].tolist()
        ids = df["id"].tolist()

        # Final check for non-string values before embedding
        for i, text in enumerate(texts):
            if not isinstance(text, str):
                texts[i] = str(text) if text is not None else ""

        embeddings, embedding_dict = generate_embeddings(texts, ids)

        # Save embeddings to files with consistent naming
        with open("kaggle_cv_embeddings.pkl", "wb") as f:
            pickle.dump(embeddings, f)

        # Save embeddings with IDs to file
        with open("kaggle_cv_embeddings_with_ids.pkl", "wb") as f:
            pickle.dump(embedding_dict, f)

        print(f"Generated and saved embeddings for {len(embeddings)} CV documents")
        print(f"Saved ID-to-embedding mapping for later training use")

    # Analyze job category distribution
    print("\nJob category distribution:")
    job_dist = df["job_category"].value_counts()
    for job, count in job_dist.items():
        print(f"  {job}: {count}")

    # Save processed data to CSV with a clear, consistent name
    df.to_csv("kaggle_processed_cv_data.csv", index=False)
    print("Processed data saved to kaggle_processed_cv_data.csv")

    return df, embeddings, embedding_dict

# Main execution
# First, make sure we install the necessary packages
!pip install pymupdf sentence-transformers

# Set the base directory to point to the dataset with job categories
base_dir = "/kaggle/input/cvs-keggle/keggleee/data/data"

# Process the dataset
df, embeddings, embedding_dict = process_cv_dataset(base_dir)

# Display sample information
if len(df) > 0:
    # First sample
    print("\n" + "="*80)
    print("SAMPLE CV #1")
    print("="*80)
    sample1 = df.iloc[0]
    print(f"ID: {sample1['id']}")
    print(f"Job Category: {sample1['job_category']}")
    print(f"Text length: {len(sample1['full_text'])} characters")
    print("\nSAMPLE TEXT (first 500 chars):")
    print("-"*80)
    print(sample1["full_text"][:500])
    print("-"*80)

    # Second sample - pick a different job category if possible
    if len(df) > 1:
        print("\n" + "="*80)
        print("SAMPLE CV #2")
        print("="*80)
        # Try to find a sample from a different job category
        job_cat1 = sample1['job_category']
        diff_job_sample = df[df['job_category'] != job_cat1]
        if len(diff_job_sample) > 0:
            sample2 = diff_job_sample.iloc[0]
        else:
            # If all samples are from the same job category, pick a different one
            sample2 = df.iloc[min(5, len(df)-1)]

        print(f"ID: {sample2['id']}")
        print(f"Job Category: {sample2['job_category']}")
        print(f"Text length: {len(sample2['full_text'])} characters")
        print("\nSAMPLE TEXT (first 500 chars):")
        print("-"*80)
        print(sample2["full_text"][:500])
        print("-"*80)

    # Display embedding information
    if embeddings is not None and len(embeddings) > 0:
        print(f"\nEmbedding shape: {embeddings[0].shape}")
        print(f"Embedding sample (first 5 values): {embeddings[0][:5]}")
        print(f"Number of IDs in embedding dictionary: {len(embedding_dict)}")
        if embedding_dict:
            sample_id = list(embedding_dict.keys())[0]
            print(f"Sample ID in embedding dictionary: {sample_id}")

# Save ID to job category mapping for future reference
id_job_mapping = df[['id', 'job_category']].copy()
id_job_mapping.to_csv("kaggle_id_job_mapping.csv", index=False)
print("\nID to job category mapping saved to kaggle_id_job_mapping.csv")

# Show overall statistics
print("\nOverall Dataset Statistics:")
print(f"Total CVs processed: {len(df)}")
print(f"Number of job categories: {df['job_category'].nunique()}")
print(f"Average text length: {df['full_text'].str.len().mean():.0f} characters")
print(f"Minimum text length: {df['full_text'].str.len().min()} characters")
print(f"Maximum text length: {df['full_text'].str.len().max()} characters")

import pandas as pd

# Load the processed CV dataset
df = pd.read_csv("kaggle_processed_cv_data.csv")

# Define the job category to cluster mapping
category_mapping = {
    "ACCOUNTANT": "High-Skill White Collar",
    "ADVOCATE": "High-Skill White Collar",
    "AGRICULTURE": "Other",
    "APPAREL": "Other",
    "ARTS": "High-Skill White Collar",
    "AUTOMOBILE": "Other",
    "AVIATION": "High-Skill White Collar",
    "BANKING": "High-Skill White Collar",
    "BPO": "Other",
    "BUSINESS-DEVELOPMENT": "High-Skill White Collar",
    "CHEF": "Other",
    "CONSTRUCTION": "Other",
    "CONSULTANT": "High-Skill White Collar",
    "DESIGNER": "High-Skill White Collar",
    "DIGITAL-MEDIA": "High-Skill White Collar",
    "ENGINEERING": "High-Skill White Collar",
    "FINANCE": "High-Skill White Collar",
    "FITNESS": "High-Skill White Collar",
    "HEALTHCARE": "High-Skill White Collar",
    "HR": "High-Skill White Collar",
    "INFORMATION-TECHNOLOGY": "High-Skill White Collar",
    "PUBLIC-RELATIONS": "High-Skill White Collar",
    "SALES": "Other",
    "TEACHER": "High-Skill White Collar"
}

# Apply the mapping to create the new job_cluster column
df['job_cluster'] = df['job_category'].map(category_mapping)

# Handle any categories that might not be in the mapping (if any)
if df['job_cluster'].isna().any():
    print(f"Warning: {df['job_cluster'].isna().sum()} records have job categories not found in the mapping.")
    print("Unique job categories without mappings:")
    unmapped_categories = df.loc[df['job_cluster'].isna(), 'job_category'].unique()
    print(unmapped_categories)
    # Assign "Other" as default for any unmapped categories
    df['job_cluster'] = df['job_cluster'].fillna("Other")

# Print dataset statistics after mapping
print("\nJob Cluster Distribution:")
cluster_dist = df['job_cluster'].value_counts()
for cluster, count in cluster_dist.items():
    print(f"  {cluster}: {count} ({count/len(df)*100:.1f}%)")

# Verify some sample records
print("\nSample records with their job clusters:")
sample_df = df.sample(min(5, len(df)))
for _, row in sample_df.iterrows():
    print(f"ID: {row['id']}, Job Category: {row['job_category']}, Job Cluster: {row['job_cluster']}")

# Save the updated dataset
df.to_csv("cv_data_with_clusters.csv", index=False)
print("\nUpdated dataset saved to cv_data_with_clusters.csv")

# Calculate some statistics about the clusters
high_skill_count = (df['job_cluster'] == "High-Skill White Collar").sum()
other_count = (df['job_cluster'] == "Other").sum()

print(f"\nTotal CVs in High-Skill White Collar: {high_skill_count} ({high_skill_count/len(df)*100:.1f}%)")
print(f"Total CVs in Other: {other_count} ({other_count/len(df)*100:.1f}%)")

# Check for any potential issues in the data
if df['id'].duplicated().any():
    print(f"\nWarning: {df['id'].duplicated().sum()} duplicate IDs found in the dataset.")

# Display the final dataset structure
print("\nFinal dataset structure:")
print(df.head())

"""### remapping into ICT, keeps white collar, it just adds a column"""

# REMAPPING INTO ICT NON-ICT

# Define the new mapping for ICT vs non-ICT jobs
ict_mapping = {
    "ACCOUNTANT": "non-ICT",
    "ADVOCATE": "non-ICT",
    "AGRICULTURE": "non-ICT",
    "APPAREL": "non-ICT",
    "ARTS": "non-ICT",
    "AUTOMOBILE": "non-ICT",
    "AVIATION": "non-ICT",
    "BANKING": "non-ICT",
    "BPO": "ICT",  # Business Process Outsourcing often involves ICT services
    "BUSINESS-DEVELOPMENT": "non-ICT",
    "CHEF": "non-ICT",
    "CONSTRUCTION": "non-ICT",
    "CONSULTANT": "non-ICT",
    "DESIGNER": "non-ICT",
    "DIGITAL-MEDIA": "ICT",  # Digital media involves ICT technologies
    "ENGINEERING": "ICT",  # changed!
    "FINANCE": "non-ICT",
    "FITNESS": "non-ICT",
    "HEALTHCARE": "non-ICT",
    "HR": "non-ICT",
    "INFORMATION-TECHNOLOGY": "ICT",  # Core ICT field
    "PUBLIC-RELATIONS": "non-ICT",
    "SALES": "non-ICT",
    "TEACHER": "non-ICT"
}

# Apply the ICT mapping to create the new job_cluster_ICT column
df['job_cluster_ICT'] = df['job_category'].map(ict_mapping)

# Handle any categories that might not be in the mapping (if any)
if df['job_cluster_ICT'].isna().any():
    print(f"Warning: {df['job_cluster_ICT'].isna().sum()} records have job categories not found in the ICT mapping.")
    print("Unique job categories without ICT mappings:")
    unmapped_categories = df.loc[df['job_cluster_ICT'].isna(), 'job_category'].unique()
    print(unmapped_categories)
    # Assign "non-ICT" as default for any unmapped categories
    df['job_cluster_ICT'] = df['job_cluster_ICT'].fillna("non-ICT")

# Print dataset statistics for the new ICT classification
print("\nJob ICT Cluster Distribution:")
ict_dist = df['job_cluster_ICT'].value_counts()
for cluster, count in ict_dist.items():
    print(f"  {cluster}: {count} ({count/len(df)*100:.1f}%)")

# Verify some sample records with both cluster types
print("\nSample records with both job clusters:")
sample_df = df.sample(min(5, len(df)))
for _, row in sample_df.iterrows():
    print(f"ID: {row['id']}, Job Category: {row['job_category']}, Original Cluster: {row['job_cluster']}, ICT Cluster: {row['job_cluster_ICT']}")

# Calculate statistics about the ICT clusters
ict_count = (df['job_cluster_ICT'] == "ICT").sum()
non_ict_count = (df['job_cluster_ICT'] == "non-ICT").sum()

print(f"\nTotal CVs in ICT: {ict_count} ({ict_count/len(df)*100:.1f}%)")
print(f"Total CVs in non-ICT: {non_ict_count} ({non_ict_count/len(df)*100:.1f}%)")

# Save the updated dataset with the new ICT classification
df.to_csv("cv_data_with_ict_clusters.csv", index=False)
print("\nUpdated dataset with ICT classification saved to cv_data_with_ict_clusters.csv")

# Display the final dataset structure with both classifications
print("\nFinal dataset structure with ICT classification:")
print(df.head())

# adding ICT columns to prof dataset
import pandas as pd

# Load the dataset
df = pd.read_csv('/kaggle/input/processed-prof-dataset-translated/processed_prof_dataset_ (5)_translated (3).csv')


# Create a mapping from text categories to ICT/non-ICT
category_to_ict = {
    # English categories
    "Clerical support workers": "non-ICT",
    "Service and sales workers": "non-ICT",
    "Craft and related trade workers": "non-ICT",
    "Associate professionals": "ICT",
    "Managers": "non-ICT",
    "Professionals": "ICT",
    "Plant and machine operators and assemblers": "non-ICT",
    "Elementary occupations": "non-ICT",
    "Skilled agricultural, forestry, and fishery workers": "non-ICT",
    "Armed forces": "non-ICT",

    # English variations with more specific descriptions
    "Managers (managers, official, legislators)": "non-ICT",
    "Professionals (health, legal, business, engineering, teaching)": "ICT",
    "Associate professionals (health, legal, business)": "non-ICT",

    # German categories
    "Akademische Berufe (Naturwissenschaft, Ingenieurswesen, Gesundheit, Bildung, Verwaltung, Wirtschaft, Wissenschaft, Technologie, Medien, Jura, Sozialwissenschaft, Kultur)": "ICT",
    "FÃ¼hrungskrÃ¤fte (diverse Bereiche)": "ICT",
    "Dienstleistungsberufe, Sicherheit und Verkauf": "non-ICT",
    "BÃ¼rokrÃ¤fte und verwandte Berufe (Administration, Kundendienst, Buchhaltung)": "non-ICT",
    "Nichtakademische Techniker*innen und gleichrangige nichttechnische Berufe (Ingenieurswesen, Gesundheit, Verwaltung, Technologie, Wirtschaft, Soziales, Kultur)": "ICT",
    "Bedienung von Anlagen und Maschinen, Montage": "non-ICT",

    # Spanish/Catalan categories
    "Personal de apoyo administrativo": "non-ICT",
    "Treballadors de suport administratiu": "non-ICT"
}

# # Apply the ICT mapping to create the new job_cluster_ICT column
# df['job_cluster_ICT'] = df['job_category'].map(ict_mapping)
# # Function to map job category to ICT/non-ICT
# def map_to_ict_text(job_category):
#     if pd.notna(job_category) and job_category in category_to_ict:
#         return category_to_ict[job_category]
#     return "non-ICT"  # Default case

# Apply the mapping to create or update the ICT column
df['job_clusters_ICT'] = df['job_category'].map(category_to_ict).fillna('non-ICT')

# Calculate and display the percentage of ICT and non-ICT jobs
ict_counts = df['job_clusters_ICT'].value_counts()
ict_percentage = df['job_clusters_ICT'].value_counts(normalize=True) * 100

# Verify the mapping worked
print("ICT Distribution:")
print(df['job_clusters_ICT'].value_counts())
print()

print("Distribution of ICT vs non-ICT jobs:")
print(f"ICT: {ict_counts.get('ICT', 0)} ({ict_percentage.get('ICT', 0):.2f}%)")
print(f"non-ICT: {ict_counts.get('non-ICT', 0)} ({ict_percentage.get('non-ICT', 0):.2f}%)")

# Save the updated dataframe
df.to_csv('/kaggle/working/processed_prof_dataset_with_ict.csv', index=False)

# Group by Gender and job_clusters_ICT to get counts
gender_ict_counts = df.groupby(['Gender', 'job_clusters_ICT']).size().unstack(fill_value=0)

print('hghhg')

# Calculate proportions within each gender
gender_ict_proportions = df.groupby(['Gender', 'job_clusters_ICT']).size().groupby(level=0).apply(lambda x: x / x.sum() * 100).unstack(fill_value=0)

print("=== ICT Distribution by Gender ===\n")

print("Absolute counts:")
print(gender_ict_counts)
print()

print("Proportions (%):")
print(gender_ict_proportions.round(2))
print()

# Get total counts by gender for reference
gender_totals = df['Gender'].value_counts()
print("Total counts by gender:")
print(gender_totals)
print()

# Summary statistics
print("=== Summary ===")
for gender in df['Gender'].unique():
    if pd.notna(gender):
        gender_data = df[df['Gender'] == gender]
        total_count = len(gender_data)
        ict_count = len(gender_data[gender_data['job_clusters_ICT'] == 'ICT'])
        ict_percentage = (ict_count / total_count) * 100 if total_count > 0 else 0

        print(f"{gender}:")
        print(f"  Total: {total_count}")
        print(f"  ICT: {ict_count} ({ict_percentage:.2f}%)")
        print(f"  non-ICT: {total_count - ict_count} ({100 - ict_percentage:.2f}%)")
        print()

"""# Phase 2, Running the Transfer Learning Model on the LiveCarrer Resumes

### for now a NN,
"""

# Alternative GPU-Optimized Approach with Native PyTorch
import os
import subprocess
import sys
import time
from datetime import datetime, timedelta
from collections import Counter
import random

# CRITICAL: Clear all GPU memory from previous runs FIRST
import torch
if torch.cuda.is_available():
    print("ðŸ§¹ Clearing ALL GPU memory from previous runs...")
    torch.cuda.empty_cache()
    # Force garbage collection
    import gc
    gc.collect()
    # Clear memory more aggressively
    for i in range(torch.cuda.device_count()):
        torch.cuda.set_device(i)
        torch.cuda.empty_cache()
    print("âœ… GPU memory cleared!")

# Force GPU usage without complex multiprocessing
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# Try a simpler import approach to avoid dependency issues
import pandas as pd
import numpy as np
import pickle
import re
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Import sklearn modules individually to avoid potential import errors
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.metrics import confusion_matrix

# Import transformers after other imports
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# GPU setup and verification
def setup_gpu():
    if not torch.cuda.is_available():
        print("âŒ No GPU available!")
        return torch.device('cpu'), False

    try:
        device = torch.device('cuda:0')
        torch.cuda.set_device(device)
    except RuntimeError as e:
        print(f"âŒ Error setting CUDA device: {e}. Falling back to CPU.")
        return torch.device('cpu'), False

    print("ðŸ§ª Comprehensive GPU test...")
    try:
        # Test 1: Basic operations
        x = torch.randn(1000, 1000, device=device)
        y = torch.mm(x, x)
        print("âœ… Basic GPU operations: OK")

        # Test 2: Gradient computation
        x.requires_grad_(True)
        z = (y * x).sum()
        z.backward()
        print("âœ… GPU gradient computation: OK")

        # Test 3: Memory allocation
        large_tensor = torch.randn(5000, 5000, device=device)
        del large_tensor, x, y, z
        torch.cuda.empty_cache()
        print("âœ… GPU memory management: OK")

        # Print GPU info
        gpu_name = torch.cuda.get_device_name(0)
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ðŸŽ® GPU: {gpu_name}")
        print(f"ðŸ’¾ Total Memory: {total_memory:.1f} GB")

        return device, True
    except Exception as e:
        print(f"âŒ GPU test failed: {e}")
        return torch.device('cpu'), False

# Set random seeds
def set_seed(seed=42):
    np.random.seed(seed)
    torch.manual_seed(seed)
    random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed(42)
device, gpu_available = setup_gpu()

# Combined under-sampling of majority class and over-sampling of minority class
def hybrid_sampling(texts, labels, target_ratio=0.25, random_state=42):
    """
    Hybrid approach: Undersample majority class + oversample minority class

    Args:
        texts: List of text samples
        labels: List of corresponding labels
        target_ratio: Target ratio of minority to majority class
        random_state: Random seed for reproducibility

    Returns:
        Tuple of (resampled_texts, resampled_labels)
    """
    random.seed(random_state)
    np.random.seed(random_state)

    # Convert to numpy arrays for easier manipulation
    texts_array = np.array(texts)
    labels_array = np.array(labels)

    # Count class distribution
    class_counts = Counter(labels)
    majority_class = max(class_counts, key=class_counts.get)
    minority_class = min(class_counts, key=class_counts.get)

    majority_count = class_counts[majority_class]
    minority_count = class_counts[minority_class]

    print(f"ðŸ“Š Original class distribution:")
    print(f"    Majority class ({majority_class}): {majority_count} samples")
    print(f"    Minority class ({minority_class}): {minority_count} samples")
    print(f"    Class imbalance ratio: {majority_count/minority_count:.2f}:1")

    # Step 1: Undersample majority class by removing some samples
    # We'll keep majority_count * undersampling_ratio samples
    undersampling_ratio = 0.6  # Keep 60% of majority samples
    new_majority_count = int(majority_count * undersampling_ratio)

    # Get indices of majority class
    majority_indices = np.where(labels_array == majority_class)[0]
    # Randomly select indices to keep
    majority_indices_to_keep = np.random.choice(majority_indices, size=new_majority_count, replace=False)

    # Step 2: Oversample minority class
    # Calculate how many minority samples we need
    target_minority_count = int(new_majority_count * target_ratio)
    samples_to_add = max(0, target_minority_count - minority_count)

    # Get indices of minority class
    minority_indices = np.where(labels_array == minority_class)[0]

    # Randomly sample indices to duplicate (with replacement)
    additional_indices = np.random.choice(minority_indices, size=samples_to_add, replace=True)

    # Create resampled dataset
    # Combine kept majority samples and all original minority samples
    indices_to_keep = np.concatenate([majority_indices_to_keep, minority_indices])
    resampled_texts = texts_array[indices_to_keep].tolist()
    resampled_labels = labels_array[indices_to_keep].tolist()

    # Add oversampled minority samples
    resampled_texts.extend(texts_array[additional_indices].tolist())
    resampled_labels.extend(labels_array[additional_indices].tolist())

    # Shuffle the resampled data
    combined = list(zip(resampled_texts, resampled_labels))
    random.shuffle(combined)
    resampled_texts, resampled_labels = zip(*combined)

    # Report new distribution
    new_class_counts = Counter(resampled_labels)
    print(f"ðŸ“Š After hybrid sampling:")
    print(f"    Majority class ({majority_class}): {new_class_counts[majority_class]} samples")
    print(f"    Minority class ({minority_class}): {new_class_counts[minority_class]} samples")
    print(f"    New class ratio: {new_class_counts[majority_class]/new_class_counts[minority_class]:.2f}:1")
    print(f"    Total samples: {len(resampled_texts)} (removed {majority_count - new_majority_count} majority samples, added {samples_to_add} minority samples)")

    return list(resampled_texts), list(resampled_labels)

# Calculate class weights for loss function
def calculate_class_weights(labels, device):
    """
    Calculate class weights inversely proportional to class frequencies.

    Args:
        labels: List of labels
        device: PyTorch device

    Returns:
        torch.Tensor: Class weights tensor
    """
    class_counts = Counter(labels)
    total_samples = len(labels)
    num_classes = len(class_counts)

    # Calculate weights: weight = total_samples / (num_classes * class_count)
    class_weights = {}
    for class_label in sorted(class_counts.keys()):
        weight = total_samples / (num_classes * class_counts[class_label])
        class_weights[class_label] = weight

    # Convert to tensor
    weights_tensor = torch.tensor([class_weights[i] for i in sorted(class_weights.keys())],
                                 dtype=torch.float32, device=device)

    print(f"ðŸ“Š Calculated class weights:")
    for class_label, weight in class_weights.items():
        class_name = 'ICT' if class_label == 1 else 'non-ICT'
        print(f"    {class_name} (Class {class_label}): {weight:.4f}")

    return weights_tensor

# Optimized Dataset Class with Memory Management
class OptimizedResumeDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

        print(f"ðŸ”„ Pre-tokenizing {len(texts)} texts (max_length={max_length})...")
        self.tokenized_data = []

        for i, text in enumerate(texts):
            if i % 500 == 0 and i > 0:
                print(f"    Tokenizing {i}/{len(texts)}...")

            encoding = self.tokenizer(
                str(text),
                truncation=True,
                padding='max_length',
                max_length=self.max_length,
                return_tensors='pt'
            )

            self.tokenized_data.append({
                'input_ids': encoding['input_ids'].squeeze(0),
                'attention_mask': encoding['attention_mask'].squeeze(0),
                'labels': torch.tensor(labels[i], dtype=torch.long)
            })

        print(f"âœ… Pre-tokenization complete: {len(self.tokenized_data)} samples")

    def __len__(self):
        return len(self.tokenized_data)

    def __getitem__(self, idx):
        return self.tokenized_data[idx]

# Custom Training Loop with Focal Loss and Threshold Adjustment
import torch.nn as nn
import torch.nn.functional as F

# Focal Loss Implementation with moderate gamma (2.0)
class FocalLoss(nn.Module):
    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):
        """
        Focal Loss for addressing class imbalance.

        Args:
            alpha: Weighting factor for rare class (can be scalar or tensor)
            gamma: Focusing parameter (higher gamma = more focus on hard examples)
            reduction: Specifies the reduction to apply to the output
        """
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        # Compute cross entropy
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')

        # Compute p_t
        pt = torch.exp(-ce_loss)

        # Compute alpha_t (if alpha is provided)
        if self.alpha is not None:
            if isinstance(self.alpha, (float, int)):
                alpha_t = self.alpha
            else:
                alpha_t = self.alpha[targets]
        else:
            alpha_t = 1.0

        # Compute focal loss
        focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

# Function to plot confusion matrix
def plot_confusion_matrix(cm, class_names=['non-ICT', 'ICT'], title='Confusion Matrix'):
    """
    Plot confusion matrix with seaborn heatmap.

    Args:
        cm: Confusion matrix
        class_names: List of class names
        title: Title for the plot
    """
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(title)
    plt.tight_layout()

    # Save plot to file
    save_dir = '/kaggle/working'
    if not os.path.exists(save_dir):
        os.makedirs(save_dir, exist_ok=True)

    # Create a unique filename including timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{save_dir}/{title.replace(' ', '_')}_{timestamp}.png"
    plt.savefig(filename)

    print(f"ðŸ“Š Confusion matrix saved to {filename}")
    plt.show()

class GPUTrainer:
    def __init__(self, model, device, batch_size=16, gradient_clip_val=1.0, class_weights=None, use_focal_loss=True, high_recall_mode=True):
        self.model = model.to(device)
        self.device = device
        self.batch_size = batch_size
        self.gradient_clip_val = gradient_clip_val
        self.use_focal_loss = use_focal_loss
        self.high_recall_mode = high_recall_mode  # Prioritize recall for minority class

        # Use Focal Loss or weighted CrossEntropyLoss
        if use_focal_loss:
            # Convert class weights to alpha for focal loss
            if class_weights is not None:
                alpha = class_weights.clone()
            else:
                alpha = None
            self.criterion = FocalLoss(alpha=alpha, gamma=2.0)  # Reduced gamma from 3.0 to 2.0
            print(f"âœ… Using Focal Loss with gamma=2.0")
        else:
            # Use weighted CrossEntropyLoss if class weights are provided
            if class_weights is not None:
                self.criterion = nn.CrossEntropyLoss(weight=class_weights)
                print(f"âœ… Using weighted CrossEntropyLoss with class weights")
            else:
                self.criterion = nn.CrossEntropyLoss()
                print(f"âœ… Using standard CrossEntropyLoss")

    def train_epoch(self, train_loader, optimizer, epoch):
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0

        start_time = time.time()

        for batch_idx, batch in enumerate(train_loader):
            # Explicit GPU transfer
            input_ids = batch['input_ids'].to(self.device, non_blocking=True if gpu_available else False)
            attention_mask = batch['attention_mask'].to(self.device, non_blocking=True if gpu_available else False)
            labels = batch['labels'].to(self.device, non_blocking=True if gpu_available else False)

            if batch_idx == 0 and epoch == 0:
                print(f"    ðŸ” Batch on device: {input_ids.device}")
                print(f"    ðŸ” Model on device: Next param device: {next(self.model.parameters()).device}")

            optimizer.zero_grad()

            # Forward pass
            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            loss = self.criterion(logits, labels)

            # NOTE: Removed manual L2 regularization as AdamW's weight_decay handles this properly

            # Backward pass
            loss.backward()

            # Gradient Clipping
            if self.gradient_clip_val is not None and self.gradient_clip_val > 0:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_val)

            optimizer.step()

            # Statistics
            total_loss += loss.item()
            _, predicted = torch.max(logits.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            # Progress updates
            if batch_idx % 10 == 0 or batch_idx == len(train_loader) - 1:
                elapsed = time.time() - start_time
                progress = (batch_idx + 1) / len(train_loader) * 100
                current_lr = optimizer.param_groups[0]['lr']
                print(f"    ðŸ“ˆ Batch {batch_idx+1}/{len(train_loader)} ({progress:.1f}%) | "
                      f"Loss: {loss.item():.4f} | "
                      f"Acc: {100.*correct/total:.2f}% | "
                      f"LR: {current_lr:.1e} | "
                      f"Time: {elapsed:.1f}s")

                if gpu_available:
                    memory_allocated = torch.cuda.memory_allocated(self.device) / 1024**3
                    memory_cached = torch.cuda.memory_reserved(self.device) / 1024**3
                    print(f"    ðŸ’¾ GPU Memory: {memory_allocated:.2f}GB allocated, {memory_cached:.2f}GB cached")

        avg_loss = total_loss / len(train_loader)
        accuracy = 100. * correct / total
        return avg_loss, accuracy

    def find_optimal_threshold(self, val_loader):
        """Find optimal classification threshold that maximizes F1 score or prioritizes recall."""
        self.model.eval()
        all_probs = []
        all_labels = []

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(self.device, non_blocking=True if gpu_available else False)
                attention_mask = batch['attention_mask'].to(self.device, non_blocking=True if gpu_available else False)
                labels = batch['labels'].to(self.device, non_blocking=True if gpu_available else False)

                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs.logits
                probs = F.softmax(logits, dim=1)

                all_probs.extend(probs[:, 1].cpu().numpy())  # Probability of positive class
                all_labels.extend(labels.cpu().numpy())

        # Try different thresholds
        thresholds = np.arange(0.1, 0.9, 0.05)
        best_f1 = 0
        best_threshold = 0.5

        # For high recall mode
        high_recall_threshold = 0.5
        highest_recall = 0

        for threshold in thresholds:
            predictions = (np.array(all_probs) >= threshold).astype(int)
            f1 = f1_score(all_labels, predictions, average='binary', pos_label=1, zero_division=0)
            recall = recall_score(all_labels, predictions, average='binary', pos_label=1, zero_division=0)
            precision = precision_score(all_labels, predictions, average='binary', pos_label=1, zero_division=0)

            if f1 > best_f1:
                best_f1 = f1
                best_threshold = threshold

            # Find threshold that gives highest recall while maintaining at least 0.7 precision (increased from 0.3)
            if recall > highest_recall and precision >= 0.7:
                highest_recall = recall
                high_recall_threshold = threshold

        # Use high recall threshold in high recall mode, otherwise use best F1 threshold
        optimal_threshold = high_recall_threshold if self.high_recall_mode else best_threshold

        return optimal_threshold, best_f1

    def evaluate(self, val_loader, find_threshold=False, threshold=0.5):
        self.model.eval()
        total_loss = 0
        all_predictions = []
        all_labels = []
        all_probs = []

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(self.device, non_blocking=True if gpu_available else False)
                attention_mask = batch['attention_mask'].to(self.device, non_blocking=True if gpu_available else False)
                labels = batch['labels'].to(self.device, non_blocking=True if gpu_available else False)

                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs.logits
                loss = self.criterion(logits, labels)

                total_loss += loss.item()

                probs = F.softmax(logits, dim=1)
                all_probs.extend(probs[:, 1].cpu().numpy())  # Probability of positive class
                all_labels.extend(labels.cpu().numpy())

                _, predicted = torch.max(logits.data, 1)
                all_predictions.extend(predicted.cpu().numpy())

        # Convert to numpy arrays
        all_labels = np.array(all_labels)
        all_probs = np.array(all_probs)
        all_predictions = np.array(all_predictions)

        avg_loss = total_loss / len(val_loader)

        # Standard metrics with 0.5 threshold
        std_preds = (all_probs >= 0.5).astype(int)
        accuracy = accuracy_score(all_labels, std_preds) * 100
        precision = precision_score(all_labels, std_preds, average='binary', pos_label=1, zero_division=0)
        recall = recall_score(all_labels, std_preds, average='binary', pos_label=1, zero_division=0)
        f1 = f1_score(all_labels, std_preds, average='binary', pos_label=1, zero_division=0)

        # Calculate confusion matrix
        cm = confusion_matrix(all_labels, std_preds)

        # Find optimal threshold if requested
        optimal_threshold = 0.5
        optimal_preds = std_preds
        optimal_cm = cm
        optimal_metrics = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}

        if find_threshold:
            # Find optimal threshold for different criteria
            f1_thresholds = np.arange(0.1, 0.9, 0.05)
            best_f1 = 0
            best_f1_threshold = 0.5

            high_recall_threshold = 0.5
            highest_recall = 0

            for thresh in f1_thresholds:
                thresh_preds = (all_probs >= thresh).astype(int)
                thresh_f1 = f1_score(all_labels, thresh_preds, average='binary', pos_label=1, zero_division=0)
                thresh_recall = recall_score(all_labels, thresh_preds, average='binary', pos_label=1, zero_division=0)

                if thresh_f1 > best_f1:
                    best_f1 = thresh_f1
                    best_f1_threshold = thresh

                # Find threshold that gives highest recall while maintaining at least 0.7 precision
                thresh_precision = precision_score(all_labels, thresh_preds, average='binary', pos_label=1, zero_division=0)
                if thresh_recall > highest_recall and thresh_precision >= 0.7:  # Increased from 0.3 to 0.7
                    highest_recall = thresh_recall
                    high_recall_threshold = thresh

            # Use high recall threshold in high recall mode, otherwise use best F1 threshold
            optimal_threshold = high_recall_threshold if self.high_recall_mode else best_f1_threshold

            # Calculate metrics with optimal threshold
            optimal_preds = (all_probs >= optimal_threshold).astype(int)
            optimal_accuracy = accuracy_score(all_labels, optimal_preds) * 100
            optimal_precision = precision_score(all_labels, optimal_preds, average='binary', pos_label=1, zero_division=0)
            optimal_recall = recall_score(all_labels, optimal_preds, average='binary', pos_label=1, zero_division=0)
            optimal_f1 = f1_score(all_labels, optimal_preds, average='binary', pos_label=1, zero_division=0)

            optimal_cm = confusion_matrix(all_labels, optimal_preds)
            optimal_metrics = {
                'accuracy': optimal_accuracy,
                'precision': optimal_precision,
                'recall': optimal_recall,
                'f1': optimal_f1
            }

        return avg_loss, optimal_metrics, optimal_preds, all_labels, optimal_threshold, optimal_cm, all_probs

# Main training function with hybrid sampling and high recall focus
def train_with_gpu_optimization(texts, labels, n_splits=5, use_hybrid_sampling=True, use_class_weights=True, target_ratio=0.25):
    print("\nðŸš€ Starting GPU-Optimized Training with Class Balancing & Precision Focus")
    print("="*80)

    model_name = "answerdotai/ModernBERT-base"
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Determine optimal settings
    optimal_batch_size = 16
    num_workers = 0
    if gpu_available:
        total_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"    Detected GPU Total Memory: {total_memory_gb:.1f}GB")
        if total_memory_gb < 10:
             optimal_batch_size = 8
        elif total_memory_gb < 16:
             optimal_batch_size = 12
    else:
        optimal_batch_size = 8

    print(f"ðŸ”§ Configuration:")
    print(f"    Model: {model_name}")
    print(f"    Device: {device}")
    print(f"    Batch size: {optimal_batch_size}")
    print(f"    DataLoaders num_workers: {num_workers}")
    print(f"    GPU Available: {gpu_available}")
    print(f"    Use Hybrid Sampling: {use_hybrid_sampling}")
    print(f"    Use Class Weights: {use_class_weights}")
    print(f"    Focal Loss Gamma: 2.0 (balanced)")
    print(f"    Dropout Rate: 0.3 (added to model)")
    print(f"    Weight Decay: 0.02 (L2 regularization via AdamW)")
    print(f"    Min Precision Threshold: 0.7 (for optimal recall)")
    if use_hybrid_sampling:
        print(f"    Target Ratio (minority:majority): {target_ratio}")

    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    fold_results = []

    for fold, (train_idx, val_idx) in enumerate(skf.split(np.array(texts), np.array(labels))):
        print(f"\n{'='*60}")
        print(f"ðŸ”¥ FOLD {fold+1}/{n_splits}")
        print(f"{'='*60}")

        if gpu_available:
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            memory_allocated = torch.cuda.memory_allocated(device) / 1024**3
            memory_cached = torch.cuda.memory_reserved(device) / 1024**3
            print(f"ðŸ’¾ Pre-fold GPU Memory: {memory_allocated:.2f}GB allocated, {memory_cached:.2f}GB cached")

        # Use np.array for indexing compatibility with scikit-learn splits
        train_texts_fold = np.array(texts)[train_idx].tolist()
        val_texts_fold = np.array(texts)[val_idx].tolist()
        train_labels_fold = np.array(labels)[train_idx].tolist()
        val_labels_fold = np.array(labels)[val_idx].tolist()

        print(f"ðŸ“Š Original Train: {len(train_texts_fold)} samples, Val: {len(val_texts_fold)} samples")

        # Apply hybrid sampling to training data if enabled
        if use_hybrid_sampling:
            print("\nðŸ”„ Applying hybrid sampling to training data...")
            train_texts_fold, train_labels_fold = hybrid_sampling(
                train_texts_fold, train_labels_fold, target_ratio=target_ratio, random_state=42+fold
            )
            print(f"ðŸ“Š After hybrid sampling - Train: {len(train_texts_fold)} samples")

        # Calculate class weights if enabled
        class_weights = None
        if use_class_weights:
            print("\nðŸ”„ Calculating class weights...")
            class_weights = calculate_class_weights(train_labels_fold, device)

        train_dataset = OptimizedResumeDataset(train_texts_fold, train_labels_fold, tokenizer)
        val_dataset = OptimizedResumeDataset(val_texts_fold, val_labels_fold, tokenizer)

        train_loader = DataLoader(
            train_dataset, batch_size=optimal_batch_size, shuffle=True,
            num_workers=num_workers, pin_memory=gpu_available, persistent_workers=False if num_workers == 0 else True
        )
        val_loader = DataLoader(
            val_dataset, batch_size=optimal_batch_size, shuffle=False,
            num_workers=num_workers, pin_memory=gpu_available, persistent_workers=False if num_workers == 0 else True
        )

        print("ðŸ”„ Loading model with memory optimization...")
        config = {"hidden_dropout_prob": 0.3, "attention_probs_dropout_prob": 0.3}  # Increase dropout
        model = AutoModelForSequenceClassification.from_pretrained(
            model_name, num_labels=2,
            low_cpu_mem_usage=True,
            config=config
        ).to(device)

        if gpu_available:
            memory_allocated = torch.cuda.memory_allocated(device) / 1024**3
            print(f"ðŸ’¾ Post-model-load GPU Memory: {memory_allocated:.2f}GB allocated")

        # Pass class weights to trainer with focal loss option and high recall mode
        trainer = GPUTrainer(
            model, device, optimal_batch_size, gradient_clip_val=1.0,
            class_weights=class_weights, use_focal_loss=True, high_recall_mode=True
        )

        # Use AdamW with moderate weight decay (L2 regularization)
        optimizer = torch.optim.AdamW(
            model.parameters(), lr=2e-5, weight_decay=0.02, eps=1e-8  # Reduced from 0.05 to 0.02
        )

        # Enable gradient checkpointing to save memory
        if hasattr(model, 'gradient_checkpointing_enable') and gpu_available:
            try:
                model.gradient_checkpointing_enable()
                print("âœ… Gradient checkpointing enabled to save memory")
            except Exception as e:
                print(f"âš ï¸ Could not enable gradient checkpointing: {e}")

        best_f1 = 0
        patience = 3
        patience_counter = 0
        num_epochs = 3  # Reduced from 4 to 3

        print("ðŸš€ Starting training for fold...")
        for epoch in range(num_epochs):
            epoch_start_time = time.time()
            print(f"\n--- Epoch {epoch+1}/{num_epochs} ---")

            train_loss, train_acc = trainer.train_epoch(train_loader, optimizer, epoch)
            val_loss, val_metrics, val_preds, val_true, optimal_threshold, val_cm, val_probs = trainer.evaluate(val_loader, find_threshold=True)

            epoch_duration = time.time() - epoch_start_time

            print(f"ðŸ“Š Epoch {epoch+1} Results:")
            print(f"    Train - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
            print(f"    Val   - Loss: {val_loss:.4f}, Acc: {val_metrics['accuracy']:.2f}%")
            print(f"    Val ICT Class - Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}")
            print(f"    Val F1: {val_metrics['f1']:.4f}")
            print(f"    Optimal Threshold: {optimal_threshold:.3f}")
            print(f"    Time: {epoch_duration:.1f}s")

            # Use F1 as primary metric
            if val_metrics['f1'] > best_f1:
                best_f1 = val_metrics['f1']
                patience_counter = 0
                if not os.path.exists('/kaggle/working'):
                    os.makedirs('/kaggle/working', exist_ok=True)
                torch.save(model.state_dict(), f'/kaggle/working/best_model_fold_{fold+1}.pth')
                print(f"    â­ New best F1: {best_f1:.4f}. Model saved.")
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"    ðŸ›‘ Early stopping triggered after {epoch+1} epochs.")
                    break

        print(f"\nðŸ“‹ Fold {fold+1} Final Results:")
        # Load best model for this fold for final report if it was saved
        if os.path.exists(f'/kaggle/working/best_model_fold_{fold+1}.pth'):
            model.load_state_dict(torch.load(f'/kaggle/working/best_model_fold_{fold+1}.pth'))
            print("    Loaded best model for this fold to report metrics.")
            # Re-evaluate with the best model state for this fold using optimal threshold
            val_loss, val_metrics, val_preds, val_true, optimal_threshold, val_cm, val_probs = trainer.evaluate(val_loader, find_threshold=True)

        print(f"    Best Val F1 for this fold: {best_f1:.4f}")
        print(f"    Final Val F1 (with optimal threshold {optimal_threshold:.3f}): {val_metrics['f1']:.4f}")
        print(f"    Final Val metrics for ICT class (Class 1):")
        print(f"      Precision: {val_metrics['precision']:.4f}")
        print(f"      Recall: {val_metrics['recall']:.4f}")
        print(f"      F1: {val_metrics['f1']:.4f}")

        print(f"    Classification Report (with optimal threshold):")
        # Ensure target_names match the number of unique labels
        unique_labels_count = len(set(labels))
        target_names_report = [f'Class {i}' for i in range(unique_labels_count)]
        if unique_labels_count == 2:
            target_names_report = ['non-ICT', 'ICT']
        print(classification_report(val_true, val_preds, target_names=target_names_report, zero_division=0))

        # Plot confusion matrix for this fold
        print("\nðŸ“Š Fold Confusion Matrix:")
        plot_confusion_matrix(val_cm, class_names=target_names_report, title=f'Confusion Matrix - Fold {fold+1}')

        fold_results.append({
            'fold': fold+1,
            'accuracy': val_metrics['accuracy']/100,
            'precision': val_metrics['precision'],
            'recall': val_metrics['recall'],
            'f1': val_metrics['f1'],
            'optimal_threshold': optimal_threshold,
            'confusion_matrix': val_cm
        })

        del model, trainer, train_dataset, val_dataset, train_loader, val_loader, optimizer
        if gpu_available:
            torch.cuda.empty_cache()

        print(f"âœ… Fold {fold+1} completed!")

    print(f"\n{'='*80}")
    print("ðŸŽ¯ FINAL CROSS-VALIDATION RESULTS")
    print(f"{'='*80}")

    if not fold_results:
        print("âš ï¸ No fold results to aggregate. Training might have failed or was skipped.")
        return [], -1

    avg_metrics = {
        metric: np.mean([r[metric] for r in fold_results])
        for metric in ['accuracy', 'precision', 'recall', 'f1']
    }
    std_metrics = {
        metric: np.std([r[metric] for r in fold_results])
        for metric in ['accuracy', 'precision', 'recall', 'f1']
    }

    print(f"ðŸ“Š Cross-Validation Average Metrics (based on best model per fold):")
    for metric_name in ['accuracy', 'precision', 'recall', 'f1']:
        print(f"    {metric_name.capitalize()}: {avg_metrics[metric_name]:.4f} Â± {std_metrics[metric_name]:.4f}")

    # Find best fold by F1 score
    best_overall_fold_info = max(fold_results, key=lambda x: x['f1'])
    print(f"\nðŸ† Best Performing Fold (by F1 score): Fold {best_overall_fold_info['fold']} with F1: {best_overall_fold_info['f1']:.4f}")

    # Calculate average confusion matrix across all folds
    if all('confusion_matrix' in r for r in fold_results):
        #avg_cm = sum(r['confusion_matrix'] for r in fold_results) / len(fold_results)
        print("\nðŸ“Š Average Confusion Matrix across all folds:")
        unique_labels_count = len(set(labels))
        target_names = ['non-ICT', 'ICT'] if unique_labels_count == 2 else [f'Class {i}' for i in range(unique_labels_count)]
        #plot_confusion_matrix(avg_cm, class_names=target_names, title='Average Confusion Matrix - All Folds')

    return fold_results, best_overall_fold_info['fold']

# Load and preprocess data
print("ðŸ“¥ Loading data...")
# --- Create dummy data if file not found for local execution ---
data_file_path = '/kaggle/working/cv_data_with_ict_clusters.csv'
if not os.path.exists(data_file_path):
    print(f"âš ï¸ File {data_file_path} not found. Creating dummy data for demonstration.")
    if not os.path.exists('/kaggle/working'):
        os.makedirs('/kaggle/working', exist_ok=True)

    # Create a more substantial dummy dataset with class imbalance
    num_samples = 2000
    # Create imbalanced dataset (20% ICT, 80% non-ICT)
    ict_samples = int(num_samples * 0.2)
    non_ict_samples = num_samples - ict_samples

    dummy_texts_ict = [f"ict technical support engineer resume example {i}" for i in range(ict_samples)]
    dummy_texts_non_ict = [f"sales manager marketing skills cv {i}" for i in range(non_ict_samples)]
    dummy_labels_ict = ['ICT'] * ict_samples
    dummy_labels_non_ict = ['non-ICT'] * non_ict_samples

    all_dummy_texts = dummy_texts_ict + dummy_texts_non_ict
    all_dummy_labels = dummy_labels_ict + dummy_labels_non_ict

    # Shuffle the dummy data
    temp_df = pd.DataFrame({'full_text': all_dummy_texts, 'job_cluster_ICT': all_dummy_labels})
    temp_df = temp_df.sample(frac=1, random_state=42).reset_index(drop=True)

    temp_df.to_csv(data_file_path, index=False)
    cv_data = temp_df
else:
    cv_data = pd.read_csv(data_file_path)

def clean_text(text):
    if not isinstance(text, str): return ""
    text = text.lower()
    text = re.sub(r'https?://\S+|www\.\S+', ' ', text)
    text = re.sub(r'\S+@\S+', ' ', text)
    text = re.sub(r'\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}', ' ', text)
    text = re.sub(r'[^\w\s.,!?-]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

print("ðŸ”„ Preprocessing data...")
cv_data['cleaned_text'] = cv_data['full_text'].apply(clean_text)
cv_data['cleaned_text'] = cv_data['cleaned_text'].fillna("")

if 'job_cluster_ICT' not in cv_data.columns:
    print("Error: 'job_cluster_ICT' column not found in the data. Please check your CSV file.")
    sys.exit(1)

cv_data['job_cluster_binary'] = (cv_data['job_cluster_ICT'] == 'ICT').astype(int)

class_counts = cv_data['job_cluster_binary'].value_counts()
print("\nðŸ“Š Original class distribution:")
for cls_val, count in class_counts.items():
    label = 'ICT' if cls_val == 1 else 'non-ICT'
    print(f"    {label} (Class {cls_val}): {count} ({count/len(cv_data)*100:.1f}%)")

# Check for class imbalance
minority_ratio = min(class_counts.values) / max(class_counts.values)
print(f"ðŸ“Š Class imbalance ratio: {minority_ratio:.3f} (1.0 = balanced)")

texts_list = cv_data['cleaned_text'].tolist()
labels_list = cv_data['job_cluster_binary'].tolist()

# Check if lists are empty before splitting
if not texts_list or not labels_list:
    print("Error: Text or label lists are empty after preprocessing. Cannot proceed with training.")
    sys.exit(1)
if len(texts_list) != len(labels_list):
    print(f"Error: Mismatch in lengths of texts ({len(texts_list)}) and labels ({len(labels_list)}).")
    sys.exit(1)

# Stratified split for train/test
train_texts, test_texts, train_labels, test_labels = train_test_split(
    texts_list, labels_list, test_size=0.2, random_state=42, stratify=labels_list
)

print(f"\nðŸ“Š Dataset splits:")
print(f"    Training: {len(train_texts)} samples")
print(f"    Testing: {len(test_texts)} samples (held out for final evaluation)")

# Configuration for class balancing techniques
USE_HYBRID_SAMPLING = True  # Combine oversampling with undersampling
USE_CLASS_WEIGHTS = True    # Use class weights to further address imbalance
TARGET_RATIO = 0.25         # Target ratio of 1:4 (ICT:non-ICT)

print(f"\nðŸŽ¯ Class Balancing Configuration:")
print(f"    Hybrid sampling enabled: {USE_HYBRID_SAMPLING}")
print(f"    Class weights enabled: {USE_CLASS_WEIGHTS}")
if USE_HYBRID_SAMPLING:
    print(f"    Target ratio (minority:majority): {TARGET_RATIO} (1:4 ratio)")
    print(f"    Strategy: Undersample majority to 60% + oversample minority to match target ratio + focus on precision (min 0.7)")

# Run GPU-optimized training with class balancing
final_fold_results, final_best_fold = train_with_gpu_optimization(
    train_texts, train_labels, n_splits=5,
    use_hybrid_sampling=USE_HYBRID_SAMPLING,
    use_class_weights=USE_CLASS_WEIGHTS,
    target_ratio=TARGET_RATIO
)

# Save results
results_path = '/kaggle/working/gpu_fold_results_with_balancing.pkl'
if not os.path.exists('/kaggle/working'):
    os.makedirs('/kaggle/working', exist_ok=True)
with open(results_path, 'wb') as f:
    pickle.dump(final_fold_results, f)

print("\nâœ… GPU-Optimized Training with Class Balancing Complete!")
if final_fold_results:
    print(f"ðŸ† Best performing fold during cross-validation: {final_best_fold}")
    print(f"ðŸ“ Cross-validation fold results saved to {results_path}")
else:
    print("âš ï¸ Training completed, but no fold results were generated.")

# Improved Test Set Evaluation
print(f"\n{'='*80}")
print("ðŸ§ª EVALUATING ON TEST SET")
print(f"{'='*80}")

if final_fold_results and final_best_fold > 0:
    print(f"ðŸ”„ Loading best model from fold {final_best_fold}...")

    # Initialize tokenizer and model architecture
    model_name = "answerdotai/ModernBERT-base"
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Display test set class distribution
    test_class_counts = Counter(test_labels)
    print("\nðŸ“Š Test Set Class Distribution:")
    for cls_val, count in test_class_counts.items():
        class_name = 'ICT' if cls_val == 1 else 'non-ICT'
        print(f"    {class_name} (Class {cls_val}): {count} ({count/len(test_labels)*100:.1f}%)")

    # Create test dataset
    print(f"ðŸ“Š Creating test dataset with {len(test_texts)} samples...")
    test_dataset = OptimizedResumeDataset(test_texts, test_labels, tokenizer)

    # Determine optimal batch size (reusing logic from above)
    optimal_batch_size = 16
    num_workers = 0
    if gpu_available:
        total_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        if total_memory_gb < 10:
             optimal_batch_size = 8
        elif total_memory_gb < 16:
             optimal_batch_size = 12
    else:
        optimal_batch_size = 8

    # Create test dataloader
    test_loader = DataLoader(
        test_dataset, batch_size=optimal_batch_size, shuffle=False,
        num_workers=num_workers, pin_memory=gpu_available
    )

    # Load model architecture with same dropout
    config = {"hidden_dropout_prob": 0.3, "attention_probs_dropout_prob": 0.3}
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name, num_labels=2,
        low_cpu_mem_usage=True,
        config=config
    ).to(device)

    # Load best weights from cross-validation
    best_model_path = f'/kaggle/working/best_model_fold_{final_best_fold}.pth'
    if os.path.exists(best_model_path):
        model.load_state_dict(torch.load(best_model_path))
        print(f"âœ… Successfully loaded best model from {best_model_path}")

        # Get the optimal threshold from best fold
        optimal_threshold = next((fold['optimal_threshold'] for fold in final_fold_results if fold['fold'] == final_best_fold), 0.5)
        print(f"ðŸ” Using optimal threshold from best fold: {optimal_threshold:.3f}")

        # Get class weights from test data if using class weights
        test_class_weights = None
        if USE_CLASS_WEIGHTS:
            print("\nðŸ”„ Calculating test class weights for evaluation...")
            test_class_weights = calculate_class_weights(test_labels, device)

        # Initialize trainer with high recall mode and possibly class weights
        trainer = GPUTrainer(
            model, device, optimal_batch_size,
            class_weights=test_class_weights,
            use_focal_loss=True,
            high_recall_mode=True
        )

        # Evaluate on test set with both default threshold (0.5) and optimal threshold
        print("\nðŸ§ª Evaluating model on test set with default threshold (0.5)...")
        test_loss, default_metrics, default_preds, test_true, _, default_cm, test_probs = trainer.evaluate(
            test_loader, find_threshold=False, threshold=0.5
        )

        print(f"\nðŸ“Š Test Set Results with Default Threshold (0.5):")
        print(f"    Accuracy: {default_metrics['accuracy']:.2f}%")
        print(f"    ICT Class (Minority):")
        print(f"      Precision: {default_metrics['precision']:.4f}")
        print(f"      Recall: {default_metrics['recall']:.4f}")
        print(f"      F1 Score: {default_metrics['f1']:.4f}")

        # Apply optimal threshold from CV
        print(f"\nðŸ§ª Applying optimal threshold ({optimal_threshold:.3f}) from cross-validation...")
        optimal_preds = (np.array(test_probs) >= optimal_threshold).astype(int)
        optimal_accuracy = accuracy_score(test_true, optimal_preds) * 100
        optimal_precision = precision_score(test_true, optimal_preds, average='binary', pos_label=1, zero_division=0)
        optimal_recall = recall_score(test_true, optimal_preds, average='binary', pos_label=1, zero_division=0)
        optimal_f1 = f1_score(test_true, optimal_preds, average='binary', pos_label=1, zero_division=0)
        optimal_cm = confusion_matrix(test_true, optimal_preds)

        # Print test metrics with optimal threshold
        print(f"\nðŸ“Š Test Set Results with Optimal Threshold ({optimal_threshold:.3f}):")
        print(f"    Accuracy: {optimal_accuracy:.2f}%")
        print(f"    ICT Class (Minority):")
        print(f"      Precision: {optimal_precision:.4f}")
        print(f"      Recall: {optimal_recall:.4f}")
        print(f"      F1 Score: {optimal_f1:.4f}")

        # Print classification report
        print("\nðŸ“‹ Test Set Classification Report (with optimal threshold):")
        target_names = ['non-ICT', 'ICT'] if len(set(test_labels)) == 2 else [f'Class {i}' for i in range(len(set(test_labels)))]
        print(classification_report(test_true, optimal_preds, target_names=target_names, zero_division=0))

        # Plot confusion matrix with optimal threshold
        print("\nðŸ“Š Test Set Confusion Matrix (with optimal threshold):")
        plot_confusion_matrix(optimal_cm, class_names=target_names,
                             title=f'Test Set Confusion Matrix (threshold: {optimal_threshold:.3f})')

        # Compute confusion matrix improvement
        print("\nðŸ“ˆ Threshold Comparison - Confusion Matrix Improvement:")
        default_tn, default_fp, default_fn, default_tp = default_cm.ravel()
        optimal_tn, optimal_fp, optimal_fn, optimal_tp = optimal_cm.ravel()

        print(f"    Default Threshold (0.5):")
        print(f"      True Positives: {default_tp}, False Positives: {default_fp}")
        print(f"      True Negatives: {default_tn}, False Negatives: {default_fn}")

        print(f"    Optimal Threshold ({optimal_threshold:.3f}):")
        print(f"      True Positives: {optimal_tp}, False Positives: {optimal_fp}")
        print(f"      True Negatives: {optimal_tn}, False Negatives: {optimal_fn}")

        # Calculate improvement percentages
        tp_change = ((optimal_tp - default_tp) / default_tp * 100) if default_tp > 0 else float('inf')
        fp_change = ((optimal_fp - default_fp) / default_fp * 100) if default_fp > 0 else float('inf')

        print(f"    Change with Optimal Threshold:")
        print(f"      True Positives: {'+' if tp_change >= 0 else ''}{tp_change:.1f}%")
        print(f"      False Positives: {'+' if fp_change >= 0 else ''}{fp_change:.1f}%")

        # Try finding an optimal threshold directly on the test set (for comparison only)
        print("\nðŸ” Finding optimal threshold directly on test set (for reference only)...")
        thresholds = np.arange(0.1, 0.9, 0.05)
        best_f1 = 0
        best_f1_threshold = 0.5

        for thresh in thresholds:
            thresh_preds = (np.array(test_probs) >= thresh).astype(int)
            thresh_f1 = f1_score(test_true, thresh_preds, average='binary', pos_label=1, zero_division=0)

            if thresh_f1 > best_f1:
                best_f1 = thresh_f1
                best_f1_threshold = thresh

        print(f"    Test set optimal threshold: {best_f1_threshold:.3f} (F1: {best_f1:.4f})")
        print(f"    Cross-validation optimal threshold: {optimal_threshold:.3f} (F1: {optimal_f1:.4f})")

        # Save test results
        test_results = {
            'default_threshold': {
                'threshold': 0.5,
                'accuracy': default_metrics['accuracy']/100,
                'precision': default_metrics['precision'],
                'recall': default_metrics['recall'],
                'f1': default_metrics['f1'],
                'confusion_matrix': default_cm.tolist()
            },
            'optimal_threshold': {
                'threshold': optimal_threshold,
                'accuracy': optimal_accuracy/100,
                'precision': optimal_precision,
                'recall': optimal_recall,
                'f1': optimal_f1,
                'confusion_matrix': optimal_cm.tolist()
            },
            'test_optimal_threshold': {
                'threshold': best_f1_threshold,
                'f1': best_f1
            }
        }

        test_results_path = '/kaggle/working/test_results_with_thresholds.pkl'
        with open(test_results_path, 'wb') as f:
            pickle.dump(test_results, f)
        print(f"ðŸ“ Test results saved to {test_results_path}")

        # Clean up
        del model, trainer, test_dataset, test_loader
        if gpu_available:
            torch.cuda.empty_cache()
    else:
        print(f"âŒ Could not find best model at {best_model_path}. Cannot evaluate on test set.")
else:
    print("âŒ No fold results or invalid best fold. Cannot evaluate on test set.")

print("\nâœ… Evaluation Complete!")
print(f"{'='*80}")
print("ðŸ PIPELINE FINISHED SUCCESSFULLY")
print(f"{'='*80}")

#raise SystemExit()

"""# PHASE 3, SPLIT-CELLS FOR DEBIASING-GAN PIPELINE

## Cell 1: Imports, setup, and data processing classes
"""

# Cell 1 - Simplified for Balanced Binary Classification

# Imports and Setup
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold, StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, confusion_matrix
from collections import Counter
import os
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
def set_seed(seed=42):
    torch.manual_seed(seed)
    np.random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed(42)

# Check GPU availability
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ðŸ”§ Using device: {device}")

# ====================================================================================
# DATA LOADING AND PREPROCESSING
# ====================================================================================

def load_and_preprocess_data(csv_path):
    """Load and preprocess the dataset - simplified version for balanced classification"""
    print(f"ðŸ“¥ Loading dataset from: {csv_path}")

    # Check if file exists
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"Dataset file not found at: {csv_path}")

    # Load data
    df = pd.read_csv(csv_path)
    print(f"ðŸ“Š Raw data loaded: {len(df)} samples")
    print(f"ðŸ“‹ Available columns: {list(df.columns)}")

    # Determine text column (try both possible names)
    if 'full_text_english' in df.columns:
        text_col = 'full_text_english'
        print(f"âœ… Using text column: {text_col}")
    # elif 'full_text' in df.columns:
    #     text_col = 'full_text'
    #     print(f"Error, Using text column: {text_col}")
    else:
        raise ValueError("No text column found! Expected 'full_text_english' or 'full_text'")

    # Determine job cluster column (try both possible names)
    if 'job_clusters_ICT' in df.columns:
        job_col = 'job_clusters_ICT'
        print(f"âœ… Using job column: {job_col}")
    elif 'job_cluster_ICT' in df.columns:
        job_col = 'job_cluster_ICT'
        print(f"âœ… Using job column: {job_col}")
    else:
        raise ValueError("No ICT job column found! Expected 'job_clusters_ICT' or 'job_cluster_ICT'")

    # Check for Gender column
    if 'Gender' not in df.columns:
        raise ValueError("No 'Gender' column found!")

    print(f"âœ… All required columns found")

    # Clean text
    print("ðŸ”„ Cleaning text data...")
    df['cleaned_text'] = df[text_col].fillna('').astype(str)

    # Remove very short texts
    original_len = len(df)
    df = df[df['cleaned_text'].str.len() > 10]
    print(f"ðŸ“Š After removing short texts: {len(df)} samples (removed {original_len - len(df)})")

    # Convert ICT labels to binary
    print("ðŸ”„ Converting job labels to binary...")
    print(f"Unique values in {job_col}: {df[job_col].unique()}")
    df['job_binary'] = (df[job_col] == 'ICT').astype(int)

    # Handle gender - support both 'Man'/'Woman' and 'Male'/'Female' formats
    print("ðŸ”„ Converting gender labels to binary...")
    print(f"Unique values in Gender: {df['Gender'].unique()}")

    # Map various gender representations to binary
    gender_mapping = {
        'Female': 0, 'female': 0, 'F': 0, 'f': 0,
        'Male': 1, 'male': 1, 'M': 1, 'm': 1,
        'Woman': 0, 'woman': 0, 'Man': 1, 'man': 1
    }

    # Apply mapping
    df['gender_binary'] = df['Gender'].map(gender_mapping)

    # Filter to keep only successfully mapped genders
    original_len = len(df)
    df = df.dropna(subset=['gender_binary'])
    print(f"ðŸ“Š After gender filtering: {len(df)} samples (removed {original_len - len(df)})")

    # Final check
    if len(df) == 0:
        raise ValueError("No valid samples remaining after preprocessing!")

    # Check class distribution
    print("\nðŸ“Š Final Class Distribution:")
    job_dist = df['job_binary'].value_counts().sort_index()
    for job_class, count in job_dist.items():
        label = 'ICT' if job_class == 1 else 'non-ICT'
        print(f"  {label}: {count} ({count/len(df)*100:.2f}%)")

    gender_dist = df['gender_binary'].value_counts().sort_index()
    for gender_class, count in gender_dist.items():
        label = 'Male' if gender_class == 1 else 'Female'
        print(f"  {label}: {count} ({count/len(df)*100:.2f}%)")

    # Check if data is balanced (no extreme imbalance handling needed)
    ict_ratio = job_dist[1] / (job_dist[0] + job_dist[1])
    if 0.3 <= ict_ratio <= 0.7:
        print(f"âœ… Classes are reasonably balanced (ICT: {ict_ratio:.1%}) - no special handling needed")
    else:
        print(f"âš ï¸ Classes are imbalanced (ICT: {ict_ratio:.1%}) - consider balancing strategies")

    # Return the cleaned dataset with clean text column
    return df

# ====================================================================================
# DATASET CLASS FOR BALANCED CLASSIFICATION
# ====================================================================================

class ProfessionalDataset(Dataset):
    def __init__(self, texts, job_labels, gender_labels, tokenizer, max_length=256):
        self.texts = texts
        self.job_labels = job_labels
        self.gender_labels = gender_labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'job_labels': torch.tensor(self.job_labels[idx], dtype=torch.long),
            'gender_labels': torch.tensor(self.gender_labels[idx], dtype=torch.long)
        }

# ====================================================================================
# SIMPLE BALANCED CLASSIFICATION UTILITIES
# ====================================================================================

def calculate_class_weights(job_labels, weight_adjustment=1.0):
    """Calculate simple class weights for balanced binary classification"""
    job_labels = np.array(job_labels)

    # Count classes
    ict_count = np.sum(job_labels == 1)
    non_ict_count = np.sum(job_labels == 0)
    total = len(job_labels)

    print(f"\nðŸ“Š Class distribution:")
    print(f"  ICT: {ict_count} ({ict_count/total*100:.1f}%)")
    print(f"  non-ICT: {non_ict_count} ({non_ict_count/total*100:.1f}%)")

    # Calculate balanced weights
    weight_non_ict = total / (2 * non_ict_count)
    weight_ict = total / (2 * ict_count)

    # Apply optional adjustment (for slight over/under-weighting)
    weight_ict *= weight_adjustment

    class_weights = torch.tensor([weight_non_ict, weight_ict], dtype=torch.float32)

    print(f"ðŸ“Š Calculated class weights:")
    print(f"  non-ICT: {weight_non_ict:.4f}")
    print(f"  ICT: {weight_ict:.4f}")
    print(f"  Ratio (ICT/non-ICT): {weight_ict/weight_non_ict:.2f}x")

    return class_weights

def create_dataloader(dataset, batch_size=16, shuffle=True):
    """Create simple dataloader for balanced classification"""
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=0)

def create_stratified_dataloader(dataset, batch_size=16):
    """Create dataloader with stratified sampling (optional for balanced data)"""
    # Get labels for stratification
    job_labels = [dataset[i]['job_labels'].item() for i in range(len(dataset))]

    # Create stratified batches
    from torch.utils.data import WeightedRandomSampler

    # Equal weights for balanced classes
    class_counts = Counter(job_labels)
    weights = [1.0/class_counts[label] for label in job_labels]

    sampler = WeightedRandomSampler(
        weights=weights,
        num_samples=len(dataset),
        replacement=True
    )

    return DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=0)

# ====================================================================================
# FOCAL LOSS FOR IMPROVED TRAINING (OPTIONAL)
# ====================================================================================

class FocalLoss(nn.Module):
    """Focal Loss for addressing any remaining class imbalance during training"""
    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)
        pt = torch.exp(-ce_loss)
        focal_loss = (1 - pt) ** self.gamma * ce_loss

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

"""## Cell 2: Model architectures and evaluation functions"""

# Cell 2 - Updated for Balanced Binary Classification

# ====================================================================================
# FOCAL LOSS IMPLEMENTATION (OPTIONAL - SIMPLIFIED FOR BALANCED DATA)
# ====================================================================================

class FocalLoss(nn.Module):
    """Focal Loss - now optional since data is balanced"""
    def __init__(self, alpha=None, gamma=1.5, reduction='mean'):  # Reduced gamma from 2.0 to 1.5
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        self.cross_entropy = nn.CrossEntropyLoss(weight=alpha, reduction='none')

    def forward(self, inputs, targets):
        ce_loss = self.cross_entropy(inputs, targets)
        pt = torch.exp(-ce_loss)
        focal_loss = ((1 - pt) ** self.gamma) * ce_loss

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

# ====================================================================================
# THRESHOLD OPTIMIZATION (UPDATED FOR BALANCED CLASSIFICATION)
# ====================================================================================

def optimize_threshold(model, val_loader, device, is_adversarial=False):
    """Find optimal prediction threshold for balanced binary classification with fairness"""
    print("\nðŸ” Optimizing classification threshold...")
    model.eval()

    # Collect all predictions and labels
    all_job_probs = []
    all_job_labels = []
    all_gender_labels = []

    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            job_labels = batch['job_labels'].to(device)
            gender_labels = batch['gender_labels']

            if is_adversarial:
                job_logits, _ = model(input_ids, attention_mask)
            else:
                job_logits = model(input_ids, attention_mask)

            job_probs = F.softmax(job_logits, dim=1)[:, 1]  # Probability of class 1 (ICT)

            all_job_probs.extend(job_probs.cpu().numpy())
            all_job_labels.extend(job_labels.cpu().numpy())
            all_gender_labels.extend(gender_labels.numpy())

    # Try different thresholds
    thresholds = np.arange(0.2, 0.8, 0.05)
    results = []

    for threshold in thresholds:
        all_preds = (np.array(all_job_probs) > threshold).astype(int)

        # Calculate overall metrics
        accuracy = accuracy_score(all_job_labels, all_preds)

        # Per-class metrics (non-ICT=0, ICT=1)
        class_precision, class_recall, class_f1, _ = precision_recall_fscore_support(
            all_job_labels, all_preds, average=None
        )

        # Overall weighted metrics
        overall_precision, overall_recall, overall_f1, _ = precision_recall_fscore_support(
            all_job_labels, all_preds, average='weighted'
        )

        # ICT class metrics (class 1) - treated equally now
        ict_precision = class_precision[1] if len(class_precision) > 1 else 0
        ict_recall = class_recall[1] if len(class_recall) > 1 else 0
        ict_f1 = class_f1[1] if len(class_f1) > 1 else 0

        # Non-ICT class metrics (class 0)
        non_ict_precision = class_precision[0] if len(class_precision) > 0 else 0
        non_ict_recall = class_recall[0] if len(class_recall) > 0 else 0
        non_ict_f1 = class_f1[0] if len(class_f1) > 0 else 0

        # Calculate per-gender metrics for fairness
        female_idx = np.array(all_gender_labels) == 0
        male_idx = np.array(all_gender_labels) == 1

        female_preds = all_preds[female_idx]
        female_labels = np.array(all_job_labels)[female_idx]

        male_preds = all_preds[male_idx]
        male_labels = np.array(all_job_labels)[male_idx]

        # Calculate demographic parity
        female_pred_rate = female_preds.mean() if len(female_preds) > 0 else 0
        male_pred_rate = male_preds.mean() if len(male_preds) > 0 else 0
        demo_parity_diff = abs(female_pred_rate - male_pred_rate)

        # Per-gender F1
        if len(female_preds) > 0 and (0 in female_labels and 1 in female_labels):
            female_f1 = f1_score(female_labels, female_preds, average='weighted')
        else:
            female_f1 = 0

        if len(male_preds) > 0 and (0 in male_labels and 1 in male_labels):
            male_f1 = f1_score(male_labels, male_preds, average='weighted')
        else:
            male_f1 = 0

        # UPDATED: Combined score now balances overall F1 and fairness equally
        # No special treatment for ICT class - treat as standard binary classification
        combined_score = 0.7 * overall_f1 - 0.3 * demo_parity_diff

        results.append({
            'threshold': threshold,
            'accuracy': accuracy,
            'overall_precision': overall_precision,
            'overall_recall': overall_recall,
            'overall_f1': overall_f1,
            'ict_f1': ict_f1,
            'ict_precision': ict_precision,
            'ict_recall': ict_recall,
            'non_ict_f1': non_ict_f1,
            'non_ict_precision': non_ict_precision,
            'non_ict_recall': non_ict_recall,
            'demo_parity_diff': demo_parity_diff,
            'female_f1': female_f1,
            'male_f1': male_f1,
            'combined_score': combined_score
        })

        print(f"  Threshold {threshold:.2f}: Overall F1={overall_f1:.4f}, ICT F1={ict_f1:.4f}, non-ICT F1={non_ict_f1:.4f}, Parity Diff={demo_parity_diff:.4f}")

    # Find best threshold based on combined score (overall F1 + fairness)
    best_result = max(results, key=lambda x: x['combined_score'])
    best_threshold = best_result['threshold']

    print(f"\nâœ… Optimal threshold (balanced F1 + fairness): {best_threshold:.2f}")
    print(f"  Overall F1: {best_result['overall_f1']:.4f}")
    print(f"  ICT F1: {best_result['ict_f1']:.4f}")
    print(f"  non-ICT F1: {best_result['non_ict_f1']:.4f}")
    print(f"  Overall Accuracy: {best_result['accuracy']:.4f}")
    print(f"  Demographic Parity Diff: {best_result['demo_parity_diff']:.4f}")
    print(f"  Female F1: {best_result['female_f1']:.4f}")
    print(f"  Male F1: {best_result['male_f1']:.4f}")

    return best_result

# ====================================================================================
# ADVERSARIAL MODEL ARCHITECTURE
# ====================================================================================

class GradientReversalFunction(torch.autograd.Function):
    """Gradient Reversal Layer"""
    @staticmethod
    def forward(ctx, x, lambda_):
        ctx.lambda_ = lambda_
        print(f"ðŸ”„ GRL Forward: Î»={lambda_}") #debug
        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        output = grad_output.neg() * ctx.lambda_
        print(f"âš¡ GRL Backward: Î»={ctx.lambda_}, grad_norm={torch.norm(grad_output):.4f}") #debug
        return output, None

class GradientReversalLayer(nn.Module):
    def __init__(self, lambda_=1.0):
        super(GradientReversalLayer, self).__init__()
        self.lambda_ = lambda_

    def forward(self, x):
        return GradientReversalFunction.apply(x, self.lambda_)

    def set_lambda(self, lambda_):
        self.lambda_ = lambda_

def load_best_pretrained_model(device):
    """Load the best performing model from cross-validation results"""
    results_path = '/kaggle/working/gpu_fold_results_with_balancing.pkl'

    # Check if the pkl file exists first
    if not os.path.exists(results_path):
        print(f"âŒ ERROR: Cross-validation results file not found at: {results_path}")
        print(f"âŒ You must run cross-validation first to generate this file!")
        raise FileNotFoundError(f"Required pre-trained model data not found at: {results_path}")

    # Load cross-validation results
    print(f"ðŸ“Š Loading cross-validation results from: {results_path}")
    with open(results_path, 'rb') as f:
        fold_results = pickle.load(f)

    print(f"ðŸ“Š Successfully loaded results for {len(fold_results)} folds")

    # Find the fold with highest F1 score
    best_fold = max(fold_results, key=lambda x: x['f1'])
    best_fold_num = best_fold['fold']
    best_f1 = best_fold['f1']

    print(f"ðŸ† Best performing fold: {best_fold_num} (F1 = {best_f1:.4f})")

    # Load the corresponding model
    model_path = f'/kaggle/working/best_model_fold_{best_fold_num}.pth'

    if not os.path.exists(model_path):
        print(f"âŒ ERROR: Pre-trained model file not found at: {model_path}")
        print(f"âŒ The best fold was identified as {best_fold_num}, but the model file is missing!")
        raise FileNotFoundError(f"Required pre-trained model file not found at: {model_path}")

    print(f"ðŸ“¥ Loading best model from: {model_path}")

    # Create model architecture
    model = AutoModelForSequenceClassification.from_pretrained(
        "answerdotai/ModernBERT-base",
        num_labels=2,
        low_cpu_mem_usage=True
    )

    # Load trained weights
    model.load_state_dict(torch.load(model_path, map_location=device))
    print(f"âœ… Best pre-trained model (Fold {best_fold_num}, F1: {best_f1:.4f}) loaded successfully!")

    # Print all fold results for reference
    print("\nðŸ“‹ All fold results:")
    for result in sorted(fold_results, key=lambda x: x['fold']):
        print(f"  Fold {result['fold']}: F1={result['f1']:.4f}, Acc={result['accuracy']:.4f}")

    return model.to(device), best_fold_num, best_f1

class BaselineModel(nn.Module):
    """Baseline model using pre-trained ModernBERT"""
    def __init__(self, pretrained_model):
        super(BaselineModel, self).__init__()
        # Use the pre-trained model directly
        self.bert_classifier = pretrained_model

    def forward(self, input_ids, attention_mask):
        outputs = self.bert_classifier(input_ids=input_ids, attention_mask=attention_mask)
        return outputs.logits

class AdversarialModel(nn.Module):
    """Adversarial model with ModernBERT backbone + gender classifier"""
    def __init__(self, pretrained_model, num_gender_classes=2, lambda_=1.0):
        super(AdversarialModel, self).__init__()

        # Extract the ModernBERT backbone from pre-trained model
        self.bert = pretrained_model.model  # This is the ModernBertModel encoder
        self.dropout = nn.Dropout(0.4)  # Balanced dropout for regularization

        # Job classifier (copy from pre-trained model)
        self.job_classifier = pretrained_model.classifier

        # Adversarial components
        # Test, a simpler architecture
        self.gradient_reversal = GradientReversalLayer(lambda_=lambda_)
        self.gender_classifier = nn.Sequential(
        nn.Linear(self.bert.config.hidden_size, 256),
        nn.ReLU(),
        nn.Dropout(0.2),  # Less aggressive dropout
        nn.Linear(256, num_gender_classes)
        )

        # self.gradient_reversal = GradientReversalLayer(lambda_=lambda_)
        # self.gender_classifier = nn.Sequential(
        #     nn.Linear(self.bert.config.hidden_size, 512),
        #     nn.ReLU(),
        #     nn.Dropout(0.4),  # Consistent dropout
        #     nn.Linear(512, 256),
        #     nn.ReLU(),
        #     nn.Dropout(0.4),  # Consistent dropout
        #     nn.Linear(256, num_gender_classes)
        # )





    def forward(self, input_ids, attention_mask, bypass_GRL=False): # FOR NOW I KEEP GRL, SINCE FOWARD SHOULD ACTUALLY PENALIZE
        # Get ModernBERT outputs
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        # ModernBERT doesn't have pooler_output, use CLS token from last_hidden_state
        pooled_output = outputs.last_hidden_state[:, 0, :]  # Take first token (CLS)

        if self.training:
            pooled_output = self.dropout(pooled_output)

        # Job prediction (using pre-trained classifier)
        job_logits = self.job_classifier(pooled_output)

        # Gender prediction,
        if bypass_GRL:
            gender_logits = self.gender_classifier(pooled_output)  # Direct path
        else:
            reversed_features = self.gradient_reversal(pooled_output)
            gender_logits = self.gender_classifier(reversed_features)


        return job_logits, gender_logits

    def set_lambda(self, lambda_):
        self.gradient_reversal.set_lambda(lambda_)

# ====================================================================================
# EVALUATION FUNCTIONS
# ====================================================================================

def evaluate_baseline_model(model, dataloader, device, threshold=0.5, print_metrics=True, dataset_name=""):
    """Evaluate baseline model with detailed per-class metrics"""
    model.eval()
    all_preds = []
    all_probs = []
    all_labels = []
    all_gender_labels = []

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['job_labels'].to(device)
            gender_labels = batch['gender_labels']

            logits = model(input_ids, attention_mask)
            probs = F.softmax(logits, dim=1)[:, 1]  # Probability of class 1 (ICT)
            preds = (probs > threshold).long()

            all_probs.extend(probs.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_gender_labels.extend(gender_labels.numpy())

    # Overall metrics
    accuracy = accuracy_score(all_labels, all_preds)

    # Per-class metrics (non-ICT=0, ICT=1)
    precision, recall, f1, support = precision_recall_fscore_support(all_labels, all_preds, average=None)

    # Calculate per-gender metrics
    female_idx = np.array(all_gender_labels) == 0
    male_idx = np.array(all_gender_labels) == 1

    female_preds = np.array(all_preds)[female_idx]
    female_labels = np.array(all_labels)[female_idx]

    male_preds = np.array(all_preds)[male_idx]
    male_labels = np.array(all_labels)[male_idx]

    # Female metrics
    if len(female_preds) > 0 and len(np.unique(female_labels)) > 1:
        female_precision, female_recall, female_f1, _ = precision_recall_fscore_support(
            female_labels, female_preds, average=None
        )
    else:
        female_precision = female_recall = female_f1 = [0, 0]

    # Male metrics
    if len(male_preds) > 0 and len(np.unique(male_labels)) > 1:
        male_precision, male_recall, male_f1, _ = precision_recall_fscore_support(
            male_labels, male_preds, average=None
        )
    else:
        male_precision = male_recall = male_f1 = [0, 0]

    # Demographic parity
    female_pred_rate = female_preds.mean() if len(female_preds) > 0 else 0
    male_pred_rate = male_preds.mean() if len(male_preds) > 0 else 0
    demo_parity_diff = abs(female_pred_rate - male_pred_rate)

    # Generate confusion matrix
    cm = confusion_matrix(all_labels, all_preds)

    if print_metrics:
        print(f"\nðŸ“Š {dataset_name} Evaluation Results (Baseline Model)")
        print("-" * 60)
        print(f"Overall Accuracy: {accuracy:.4f}")
        print(f"Non-ICT (Class 0) - Precision: {precision[0]:.4f}, Recall: {recall[0]:.4f}, F1: {f1[0]:.4f}, Support: {support[0]}")
        print(f"ICT (Class 1) - Precision: {precision[1]:.4f}, Recall: {recall[1]:.4f}, F1: {f1[1]:.4f}, Support: {support[1]}")
        print(f"Demographic Parity Diff: {demo_parity_diff:.4f}")
        print(f"Female Prediction Rate: {female_pred_rate:.4f}")
        print(f"Male Prediction Rate: {male_pred_rate:.4f}")

        # Check for trivial classifier
        if cm[0, 0] == 0 and cm[1, 0] == 0:
            print("âš ï¸ WARNING: Model is predicting ALL samples as ICT!")
        elif cm[0, 1] == 0 and cm[1, 1] == 0:
            print("âš ï¸ WARNING: Model is predicting ALL samples as non-ICT!")

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'support': support,
        'female_precision': female_precision,
        'female_recall': female_recall,
        'female_f1': female_f1,
        'male_precision': male_precision,
        'male_recall': male_recall,
        'male_f1': male_f1,
        'demo_parity_diff': demo_parity_diff,
        'female_pred_rate': female_pred_rate,
        'male_pred_rate': male_pred_rate,
        'confusion_matrix': cm,
        'all_probs': all_probs,
        'all_preds': all_preds,
        'all_labels': all_labels,
        'all_gender_labels': all_gender_labels
    }

def evaluate_adversarial_model(model, dataloader, device, threshold=0.5, print_metrics=True, dataset_name=""):
    """Evaluate adversarial model with detailed per-class metrics"""
    model.eval()
    all_job_preds = []
    all_job_probs = []
    all_job_labels = []
    all_gender_preds = []
    all_gender_labels = []

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            job_labels = batch['job_labels'].to(device)
            gender_labels = batch['gender_labels'].to(device)

            job_logits, gender_logits = model(input_ids, attention_mask)
            job_probs = F.softmax(job_logits, dim=1)[:, 1]  # Probability of class 1 (ICT)
            job_preds = (job_probs > threshold).long()
            gender_preds = torch.argmax(gender_logits, dim=1)

            all_job_probs.extend(job_probs.cpu().numpy())
            all_job_preds.extend(job_preds.cpu().numpy())
            all_job_labels.extend(job_labels.cpu().numpy())
            all_gender_preds.extend(gender_preds.cpu().numpy())
            all_gender_labels.extend(gender_labels.cpu().numpy())

    # Overall metrics
    job_accuracy = accuracy_score(all_job_labels, all_job_preds)
    gender_accuracy = accuracy_score(all_gender_labels, all_gender_preds)

    # Per-class metrics (non-ICT=0, ICT=1)
    precision, recall, f1, support = precision_recall_fscore_support(all_job_labels, all_job_preds, average=None)

    # Calculate per-gender metrics
    female_idx = np.array(all_gender_labels) == 0
    male_idx = np.array(all_gender_labels) == 1

    female_preds = np.array(all_job_preds)[female_idx]
    female_labels = np.array(all_job_labels)[female_idx]

    male_preds = np.array(all_job_preds)[male_idx]
    male_labels = np.array(all_job_labels)[male_idx]

    # Female metrics
    if len(female_preds) > 0 and len(np.unique(female_labels)) > 1:
        female_precision, female_recall, female_f1, _ = precision_recall_fscore_support(
            female_labels, female_preds, average=None
        )
    else:
        female_precision = female_recall = female_f1 = [0, 0]

    # Male metrics
    if len(male_preds) > 0 and len(np.unique(male_labels)) > 1:
        male_precision, male_recall, male_f1, _ = precision_recall_fscore_support(
            male_labels, male_preds, average=None
        )
    else:
        male_precision = male_recall = male_f1 = [0, 0]

    # Demographic parity
    female_pred_rate = female_preds.mean() if len(female_preds) > 0 else 0
    male_pred_rate = male_preds.mean() if len(male_preds) > 0 else 0
    demo_parity_diff = abs(female_pred_rate - male_pred_rate)

    # Generate confusion matrix
    cm = confusion_matrix(all_job_labels, all_job_preds)

    if print_metrics:
        print(f"\nðŸ“Š {dataset_name} Evaluation Results (Adversarial Model)")
        print("-" * 60)
        print(f"Job Accuracy: {job_accuracy:.4f}")
        print(f"Gender Accuracy: {gender_accuracy:.4f} (closer to 0.5 is better)")
        print(f"Non-ICT (Class 0) - Precision: {precision[0]:.4f}, Recall: {recall[0]:.4f}, F1: {f1[0]:.4f}, Support: {support[0]}")
        print(f"ICT (Class 1) - Precision: {precision[1]:.4f}, Recall: {recall[1]:.4f}, F1: {f1[1]:.4f}, Support: {support[1]}")
        print(f"Demographic Parity Diff: {demo_parity_diff:.4f}")
        print(f"Female Prediction Rate: {female_pred_rate:.4f}")
        print(f"Male Prediction Rate: {male_pred_rate:.4f}")

        # Check for trivial classifier
        if cm[0, 0] == 0 and cm[1, 0] == 0:
            print("âš ï¸ WARNING: Model is predicting ALL samples as ICT!")
        elif cm[0, 1] == 0 and cm[1, 1] == 0:
            print("âš ï¸ WARNING: Model is predicting ALL samples as non-ICT!")

    return {
        'job_accuracy': job_accuracy,
        'gender_accuracy': gender_accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'support': support,
        'female_precision': female_precision,
        'female_recall': female_recall,
        'female_f1': female_f1,
        'male_precision': male_precision,
        'male_recall': male_recall,
        'male_f1': male_f1,
        'demo_parity_diff': demo_parity_diff,
        'female_pred_rate': female_pred_rate,
        'male_pred_rate': male_pred_rate,
        'confusion_matrix': cm,
        'all_job_probs': all_job_probs,
        'all_job_preds': all_job_preds,
        'all_job_labels': all_job_labels,
        'all_gender_preds': all_gender_preds,
        'all_gender_labels': all_gender_labels
    }

# ====================================================================================
# VISUALIZATION FUNCTIONS
# ====================================================================================

def plot_confusion_matrix(cm, classes=['non-ICT', 'ICT'], title='Confusion Matrix'):
    """Plot confusion matrix using seaborn"""
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(title)
    plt.ylabel('True')
    plt.xlabel('Predicted')
    plt.xticks(np.arange(len(classes)) + 0.5, classes)
    plt.yticks(np.arange(len(classes)) + 0.5, classes)
    plt.tight_layout()

    # Check for trivial classifier
    if cm[0, 0] == 0 and cm[1, 0] == 0:
        plt.figtext(0.5, 0.01, "WARNING: Model is predicting ALL samples as ICT!",
                   ha="center", fontsize=12, bbox={"facecolor":"red", "alpha":0.2})
    elif cm[0, 1] == 0 and cm[1, 1] == 0:
        plt.figtext(0.5, 0.01, "WARNING: Model is predicting ALL samples as non-ICT!",
                   ha="center", fontsize=12, bbox={"facecolor":"red", "alpha":0.2})

    plt.show()

def plot_f1_comparison(train_metrics, test_metrics, title):
    """Plot per-class F1 scores for train vs test"""
    classes = ['non-ICT', 'ICT']

    # Extract F1 scores for each class
    train_f1 = train_metrics['f1']
    test_f1 = test_metrics['f1']

    plt.figure(figsize=(10, 6))
    x = np.arange(len(classes))
    width = 0.35

    plt.bar(x - width/2, train_f1, width, label='Train')
    plt.bar(x + width/2, test_f1, width, label='Test')

    plt.xlabel('Class')
    plt.ylabel('F1 Score')
    plt.title(f'F1 Score Comparison: {title}')
    plt.xticks(x, classes)
    plt.legend()
    plt.ylim(0, 1.0)

    # Add value labels
    for i, v in enumerate(train_f1):
        plt.text(i - width/2, v + 0.02, f'{v:.2f}', ha='center')
    for i, v in enumerate(test_f1):
        plt.text(i + width/2, v + 0.02, f'{v:.2f}', ha='center')

    plt.tight_layout()
    plt.show()

def plot_gender_comparison(metrics, title):
    """Plot per-gender F1 scores for each class"""
    classes = ['non-ICT', 'ICT']

    # Extract F1 scores for each gender and class
    female_f1 = metrics['female_f1']
    male_f1 = metrics['male_f1']

    plt.figure(figsize=(10, 6))
    x = np.arange(len(classes))
    width = 0.35

    plt.bar(x - width/2, female_f1, width, label='Female')
    plt.bar(x + width/2, male_f1, width, label='Male')

    plt.xlabel('Class')
    plt.ylabel('F1 Score')
    plt.title(f'F1 Score by Gender: {title}')
    plt.xticks(x, classes)
    plt.legend()
    plt.ylim(0, 1.0)

    # Add value labels
    for i, v in enumerate(female_f1):
        plt.text(i - width/2, v + 0.02, f'{v:.2f}', ha='center')
    for i, v in enumerate(male_f1):
        plt.text(i + width/2, v + 0.02, f'{v:.2f}', ha='center')

    plt.tight_layout()
    plt.show()

# ====================================================================================
# LAMBDA OPTIMIZATION WITH PRE-TRAINED ADVERSARY
# ====================================================================================

def optimize_lambda_parameter_with_pretrained_adversary(pretrained_model_state, train_dataset, val_dataset,
                                                        device, class_weights, tokenizer,
                                                        lambda_values=[5.0], max_epochs=6): #FROM 3-> 6, test | , 1.5, 2.0, 5.0
    """
    Optimize lambda parameter using a pre-trained adversary model.
    This ensures the gender predictor is already capable before testing lambda values.
    """
    print(f"\nðŸ”¬ Lambda Optimization with Pre-trained Adversary")
    print("="*60)
    print(f"Testing lambda values: {lambda_values}")
    print(f"Using pre-trained adversary model as starting point")

    results = []

    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

    for lambda_val in lambda_values:
        print(f"\nðŸ§ª Testing Î» = {lambda_val}")
        print("-" * 30)

        # Create fresh model for this lambda
        config = {
            "hidden_dropout_prob": 0.3,
            "attention_probs_dropout_prob": 0.2
        }

        base_model = AutoModelForSequenceClassification.from_pretrained(
            "answerdotai/ModernBERT-base",
            num_labels=2,
            low_cpu_mem_usage=True,
            config=config
        ).to(device)

        # Create adversarial model with current lambda
        test_model = AdversarialModel(base_model, lambda_=lambda_val).to(device)

        # Load the pre-trained adversary weights (including strong gender predictor)
        test_model.load_state_dict(pretrained_model_state)

        # Set lambda in GRL
        test_model.set_lambda(lambda_val)

        # Short training with this lambda
        job_criterion = nn.CrossEntropyLoss(weight=class_weights)
        gender_criterion = nn.CrossEntropyLoss()

        # Use SOTA-style optimizer (different LRs)
        optimizer = torch.optim.AdamW([
            {
                'params': list(test_model.bert.parameters()) + list(test_model.job_classifier.parameters()),
                'lr': 5e-6,
                'weight_decay': 0.02
            },
            {
                'params': test_model.gender_classifier.parameters(),
                'lr': 5e-4,  # Higher LR for gender classifier
                'weight_decay': 0.02
            }
        ])

        # Training loop
        for epoch in range(max_epochs):
            test_model.train()
            total_job_loss = 0
            total_gender_loss = 0



            for batch in train_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                job_labels = batch['job_labels'].to(device)
                gender_labels = batch['gender_labels'].to(device)

                optimizer.zero_grad()

                job_logits, gender_logits = test_model(input_ids, attention_mask)

                job_loss = job_criterion(job_logits, job_labels)
                gender_loss = gender_criterion(gender_logits, gender_labels)

                combined_loss = job_loss + gender_loss

                combined_loss.backward()
                torch.nn.utils.clip_grad_norm_(test_model.parameters(), max_norm=1.0)
                optimizer.step()

                total_job_loss += job_loss.item()
                total_gender_loss += gender_loss.item()

            print(f"  Epoch {epoch+1}: Job Loss={total_job_loss/len(train_loader):.4f}, Gender Loss={total_gender_loss/len(train_loader):.4f}")

        # Evaluate on validation set
        test_model.eval()
        val_job_preds = []
        val_job_labels = []
        val_gender_preds = []
        val_gender_labels = []

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                job_labels = batch['job_labels'].to(device)
                gender_labels = batch['gender_labels'].to(device)

                job_logits, gender_logits = test_model(input_ids, attention_mask)

                job_preds = torch.argmax(job_logits, dim=1)
                gender_preds = torch.argmax(gender_logits, dim=1)

                val_job_preds.extend(job_preds.cpu().numpy())
                val_job_labels.extend(job_labels.cpu().numpy())
                val_gender_preds.extend(gender_preds.cpu().numpy())
                val_gender_labels.extend(gender_labels.cpu().numpy())

        # Calculate metrics
        job_accuracy = accuracy_score(val_job_labels, val_job_preds)
        job_f1 = f1_score(val_job_labels, val_job_preds, average='weighted')
        gender_accuracy = accuracy_score(val_gender_labels, val_gender_preds)

        # Calculate fairness metrics
        female_idx = np.array(val_gender_labels) == 0
        male_idx = np.array(val_gender_labels) == 1

        female_pred_rate = np.array(val_job_preds)[female_idx].mean() if np.any(female_idx) else 0
        male_pred_rate = np.array(val_job_preds)[male_idx].mean() if np.any(male_idx) else 0
        demo_parity_diff = abs(female_pred_rate - male_pred_rate)

        # Combined score: Balance job performance and debiasing
        # Lower gender accuracy = better debiasing
        # Higher job F1 = better performance
        debiasing_score = 1.0 - 2 * abs(gender_accuracy - 0.5)  # This gives: Score = 1.0 when gender_accuracy = 0.5 (perfect uninformativeness) | Score = 0.0 when gender_accuracy = 0.0 or 1.0 (maximum bias). multiply by 2 to normalize [0, 1]
        combined_score = 0.6 * job_f1 + 0.4 * debiasing_score #custom weights

        result = {
            'lambda': lambda_val,
            'job_accuracy': job_accuracy,
            'job_f1': job_f1,
            'gender_accuracy': gender_accuracy,
            'demo_parity_diff': demo_parity_diff,
            'debiasing_score': debiasing_score,
            'combined_score': combined_score
        }

        results.append(result)

        print(f"  Results: Job F1={job_f1:.4f}, Gender Acc={gender_accuracy:.4f}, Combined Score={combined_score:.4f}")
        print(f"  Debiasing: {'Good' if gender_accuracy < 0.6 else 'Weak'}, Job Performance: {'Good' if job_f1 > 0.7 else 'Needs improvement'}")

    # Find optimal lambda
    best_result = max(results, key=lambda x: x['combined_score'])
    optimal_lambda = best_result['lambda']

    print(f"\nðŸ“Š Lambda Optimization Results:")
    print("-" * 50)
    for r in results:
        marker = " â†â† OPTIMAL" if r['lambda'] == optimal_lambda else ""
        print(f"Î»={r['lambda']:<4}: Job F1={r['job_f1']:.4f}, Gender Acc={r['gender_accuracy']:.4f}, Score={r['combined_score']:.4f}{marker}")

    print(f"\nâœ… Optimal Î» = {optimal_lambda}")
    print(f"   Job F1: {best_result['job_f1']:.4f}")
    print(f"   Gender Accuracy: {best_result['gender_accuracy']:.4f} ({'Good debiasing' if best_result['gender_accuracy'] < 0.6 else 'Weak debiasing'})")
    print(f"   Demographic Parity Diff: {best_result['demo_parity_diff']:.4f}")

    return optimal_lambda, results

def plot_lambda_optimization_results(lambda_results):
    """Plot lambda optimization results"""
    lambdas = [r['lambda'] for r in lambda_results]
    job_f1s = [r['job_f1'] for r in lambda_results]
    gender_accs = [r['gender_accuracy'] for r in lambda_results]
    combined_scores = [r['combined_score'] for r in lambda_results]

    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

    # Plot 1: Job F1 vs Lambda
    ax1.plot(lambdas, job_f1s, 'b-o', linewidth=2, markersize=8)
    ax1.set_xlabel('Lambda (Î»)')
    ax1.set_ylabel('Job F1 Score')
    ax1.set_title('Job Performance vs Lambda')
    ax1.grid(True, alpha=0.3)

    # Plot 2: Gender Accuracy vs Lambda
    ax2.plot(lambdas, gender_accs, 'r-o', linewidth=2, markersize=8)
    ax2.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='Random (50%)')
    ax2.set_xlabel('Lambda (Î»)')
    ax2.set_ylabel('Gender Prediction Accuracy')
    ax2.set_title('Gender Predictability vs Lambda\n(Lower = Better Debiasing)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # Plot 3: Combined Score vs Lambda
    ax3.plot(lambdas, combined_scores, 'g-o', linewidth=2, markersize=8)
    optimal_idx = combined_scores.index(max(combined_scores))
    ax3.scatter([lambdas[optimal_idx]], [combined_scores[optimal_idx]],
               color='red', s=100, zorder=5, label=f'Optimal Î»={lambdas[optimal_idx]}')
    ax3.set_xlabel('Lambda (Î»)')
    ax3.set_ylabel('Combined Score')
    ax3.set_title('Combined Score vs Lambda\n(Job Performance + Debiasing)')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # Plot 4: Trade-off scatter
    ax4.scatter(gender_accs, job_f1s, c=lambdas, cmap='viridis', s=100, alpha=0.7)
    for i, lambda_val in enumerate(lambdas):
        ax4.annotate(f'Î»={lambda_val}', (gender_accs[i], job_f1s[i]),
                    xytext=(5, 5), textcoords='offset points', fontsize=9)

    ax4.axvline(x=0.5, color='gray', linestyle='--', alpha=0.7, label='Random Gender Prediction')
    ax4.set_xlabel('Gender Prediction Accuracy')
    ax4.set_ylabel('Job F1 Score')
    ax4.set_title('Performance vs Debiasing Trade-off')
    ax4.legend()
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

# ====================================================================================
# STATE-OF-THE-ART ADVERSARIAL TRAINING FUNCTIONS -SOTA
# ====================================================================================

def pretrain_adversary(model, train_loader, device, epochs=10):
    """
    Pre-train the gender classifier to establish baseline gender prediction ability.
    Job classifier and BERT encoder remain frozen.
    """
    print(f"\nðŸ”§ PRE-TRAINING ADVERSARY ({epochs} epochs)")
    print("="*50)
    print("â€¢ BERT encoder: Active (frozen)")
    print("â€¢ Job classifier: Frozen")
    print("â€¢ Gender classifier: Training")

    # Freeze everything except gender classifier
    for param in model.bert.parameters():
        param.requires_grad = False
    for param in model.job_classifier.parameters():
        param.requires_grad = False
    for param in model.gender_classifier.parameters():
        param.requires_grad = True

    # Higher learning rate for pre-training
    optimizer = torch.optim.AdamW(model.gender_classifier.parameters(), lr=1e-3, weight_decay=0.01)
    criterion = nn.CrossEntropyLoss()

    results = []

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        gender_preds = []
        gender_labels = []

        print(f"\nPre-training Epoch {epoch+1}/{epochs}")

        for batch_idx, batch in enumerate(train_loader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            batch_gender_labels = batch['gender_labels'].to(device)

            optimizer.zero_grad()

            # # Forward pass - get gender predictions only
            # _, gender_logits = model(input_ids, attention_mask)
            # loss = criterion(gender_logits, batch_gender_labels)

            # Use bypass_grl=True during pre-training
            _, gender_logits = model(input_ids, attention_mask, bypass_GRL=True)
            loss = criterion(gender_logits, batch_gender_labels)

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.gender_classifier.parameters(), max_norm=1.0)
            optimizer.step()

            total_loss += loss.item()


            # Collect predictions for accuracy
            preds = torch.argmax(gender_logits, dim=1)
            gender_preds.extend(preds.cpu().numpy())
            gender_labels.extend(batch_gender_labels.cpu().numpy())

            if (batch_idx + 1) % 20 == 0:
                print(f"  Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}")

        # Calculate epoch metrics
        avg_loss = total_loss / len(train_loader)
        gender_acc = accuracy_score(gender_labels, gender_preds)


        results.append({
            'epoch': epoch + 1,
            'loss': avg_loss,
            'gender_accuracy': gender_acc
        })

        print(f"  Epoch {epoch+1} Summary:")
        print(f"    Loss: {avg_loss:.4f}")
        print(f"    Gender Accuracy: {gender_acc:.4f}")

        # Check if adversary is learning
        if gender_acc < 0.55:
            print(f"    âš ï¸ Warning: Gender accuracy still low - may need more epochs")
        else:
            print(f"    âœ… Good: Gender classifier learning successfully")

    # Unfreeze all parameters for main training
    for param in model.parameters():
        param.requires_grad = True

    final_acc = accuracy_score(gender_labels, gender_preds)
    print(f"\nâœ… Pre-training complete. Final gender accuracy: {final_acc:.4f}")
    print(f"ðŸŽ¯ Target: >0.6 for successful pre-training (yours: {final_acc:.4f})")


    return results, final_acc

def create_adversarial_optimizer(model, job_lr=5e-6, gender_lr=5e-4, weight_decay=0.02):
    """
    Create optimizer with different learning rates for job vs gender classifiers.
    Standard practice: Gender classifier needs much higher LR.
    """
    optimizer = torch.optim.AdamW([
        {
            'params': list(model.bert.parameters()) + list(model.job_classifier.parameters()),
            'lr': job_lr,
            'weight_decay': weight_decay
        },
        {
            'params': model.gender_classifier.parameters(),
            'lr': gender_lr,  # 100x higher than job LR
            'weight_decay': weight_decay
        }
    ])

    print(f"ðŸ“Š Optimizer created:")
    print(f"  â€¢ Job + BERT LR: {job_lr}")
    print(f"  â€¢ Gender LR: {gender_lr} ({gender_lr/job_lr:.0f}x higher)")

    return optimizer

def get_progressive_lambda(epoch, max_epochs, final_lambda=5.0, schedule_type="linear"): #final_lambda=1.0 is just a default value! then it get's re-assigned correctly to best one in the scheduling
    """
    Progressive lambda scheduling for adversarial training.
    """
    progress = epoch / max_epochs

    if schedule_type == "linear":
        return final_lambda * progress
    elif schedule_type == "cosine":
        return final_lambda * (1 - np.cos(np.pi * progress)) / 2
    elif schedule_type == "step":
        warmup_fraction = 0.3
        return final_lambda if progress > warmup_fraction else 0.0
    else:
        return final_lambda

def train_final_adversarial_model_sota(model, train_loader, device, class_weights,
                                       max_epochs=15, final_lambda=None, schedule_type="linear"): #from 10 EPOCHS
    """
    Train adversarial model with state-of-the-art practices:
    - Adversary pre-training
    - Different learning rates
    - Lambda scheduling
    """
    print("\nðŸš€ SOTA Adversarial Training")
    print("="*50)

    # Step 1: Pre-train adversary, REMOVE
    #pretrain_accuracy = pretrain_adversary(model, train_loader, device, epochs=3)


    # Step 2: Setup for main training
    job_criterion = nn.CrossEntropyLoss(weight=class_weights)
    gender_criterion = nn.CrossEntropyLoss()

    # Different learning rates
    optimizer = create_adversarial_optimizer(model, job_lr=5e-6, gender_lr=5e-4)

    results = []
    best_combined_loss = float('inf')
    patience = 9
    patience_counter = 0

    print(f"\nðŸŽ¯ Main adversarial training ({max_epochs} epochs)")
    print(f"ðŸ“ˆ Lambda schedule: {schedule_type} (final Î»={final_lambda})")

    for epoch in range(max_epochs):
        # Progressive lambda
        current_lambda = get_progressive_lambda(epoch, max_epochs, final_lambda, schedule_type)

        model.train()
        total_job_loss = 0
        total_gender_loss = 0
        total_combined_loss = 0
        train_job_preds = []
        train_job_labels = []

        #model.set_lambda(current_lambda)

        print(f"\nEpoch {epoch+1}/{max_epochs} (GRL Î»={current_lambda:.3f})")

        for batch_idx, batch in enumerate(train_loader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            job_labels = batch['job_labels'].to(device)
            gender_labels = batch['gender_labels'].to(device)

            optimizer.zero_grad()

            job_logits, gender_logits = model(input_ids, attention_mask)

            job_loss = job_criterion(job_logits, job_labels)
            gender_loss = gender_criterion(gender_logits, gender_labels)


            combined_loss = job_loss + gender_loss

            combined_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            total_job_loss += job_loss.item()
            total_gender_loss += gender_loss.item()
            total_combined_loss += combined_loss.item()

            job_preds = torch.argmax(job_logits, dim=1)
            train_job_preds.extend(job_preds.cpu().numpy())
            train_job_labels.extend(job_labels.cpu().numpy())

        # Calculate metrics
        avg_job_loss = total_job_loss / len(train_loader)
        avg_gender_loss = total_gender_loss / len(train_loader)
        avg_combined_loss = total_combined_loss / len(train_loader)
        train_job_acc = accuracy_score(train_job_labels, train_job_preds)

        print(f"  Job Loss: {avg_job_loss:.4f}, Gender Loss: {avg_gender_loss:.4f}")
        print(f"  Job Accuracy: {train_job_acc:.4f}, Combined Loss: {avg_combined_loss:.4f}")

        # Early stopping
        if avg_combined_loss < best_combined_loss:
            best_combined_loss = avg_combined_loss
            patience_counter = 0
            print(f"  âœ… New best combined loss: {best_combined_loss:.4f}")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"  ðŸ›‘ Early stopping: No improvement for {patience} epochs")
                break

        results.append({
            'epoch': epoch+1,
            'lambda': current_lambda,
            'job_loss': avg_job_loss,
            'gender_loss': avg_gender_loss,
            'combined_loss': avg_combined_loss,
            'train_job_acc': train_job_acc
        })

    print(f"\nâœ… SOTA adversarial training complete!")
    return results

"""### test plotting function for graphs"""

def plot_cv_learning_curves(fold_results_detailed):
    """Plot train/val accuracy curves for all 3 folds during full fine-tuning phase"""
    import matplotlib.pyplot as plt

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    for fold_idx, fold_data in enumerate(fold_results_detailed):
        # Filter only unfrozen phase data
        unfrozen_data = [epoch for epoch in fold_data if epoch['phase'] == 'unfrozen']

        if unfrozen_data:  # Make sure we have unfrozen data
            epochs = [epoch['epoch'] for epoch in unfrozen_data]
            train_acc = [epoch['train_acc'] for epoch in unfrozen_data]
            val_acc = [epoch['val_acc'] for epoch in unfrozen_data]

            axes[fold_idx].plot(epochs, train_acc, 'b-o', label='Train Accuracy', linewidth=2)
            axes[fold_idx].plot(epochs, val_acc, 'r-o', label='Val Accuracy', linewidth=2)
            axes[fold_idx].set_title(f'Fold {fold_idx+1} - Full Fine-tuning')
            axes[fold_idx].set_xlabel('Epoch')
            axes[fold_idx].set_ylabel('Accuracy')
            axes[fold_idx].legend()
            axes[fold_idx].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()


def plot_adversarial_losses(adversarial_results):
    """Plot Job Loss and Gender Loss across training epochs"""
    import matplotlib.pyplot as plt

    epochs = [result['epoch'] for result in adversarial_results]
    job_losses = [result['job_loss'] for result in adversarial_results]
    gender_losses = [result['gender_loss'] for result in adversarial_results]

    plt.figure(figsize=(10, 6))
    plt.plot(epochs, job_losses, 'b-o', label='Job Loss', linewidth=2, markersize=6)
    plt.plot(epochs, gender_losses, 'r-o', label='Gender Loss', linewidth=2, markersize=6)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Adversarial Training: Job vs Gender Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()


def plot_adversary_pretraining(pretrain_results):
    """Plot Loss and Gender Accuracy progression during adversary pre-training"""
    import matplotlib.pyplot as plt

    epochs = [result['epoch'] for result in pretrain_results]
    losses = [result['loss'] for result in pretrain_results]
    gender_accs = [result['gender_accuracy'] for result in pretrain_results]

    plt.figure(figsize=(10, 6))
    plt.plot(epochs, losses, 'r-o', label='Loss', linewidth=2, markersize=6)
    plt.plot(epochs, gender_accs, 'b-o', label='Gender Accuracy', linewidth=2, markersize=6)
    plt.xlabel('Epoch')
    plt.ylabel('Loss / Accuracy')
    plt.title('Adversary Pre-training: Loss and Gender Accuracy Progression')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

"""## Cell 3: Training functions and main execution"""

# # ====================================================================================
# # MAIN EXECUTION - BALANCED BINARY CLASSIFICATION WITH ADVERSARIAL DEBIASING
# # ====================================================================================

# def main():
#     print("ðŸš€ Balanced Debiasing Pipeline - Standard Binary Classification")
#     print("="*90)

#     # Load data
#     csv_path = '/kaggle/working/processed_prof_dataset_with_ict.csv'
#     df = load_and_preprocess_data(csv_path)

#     # Extract texts and labels from the cleaned dataframe
#     texts = df['cleaned_text'].tolist()
#     job_labels = df['job_binary'].tolist()
#     gender_labels = df['gender_binary'].tolist()

#     print(f"\nðŸ“Š Dataset overview:")
#     print(f"  Total samples: {len(texts)}")
#     print(f"  ICT jobs: {sum(job_labels)} ({sum(job_labels)/len(job_labels)*100:.1f}%)")
#     print(f"  Male samples: {sum(gender_labels)} ({sum(gender_labels)/len(gender_labels)*100:.1f}%)")
#     print(f"âœ… Classes are balanced - no special minority handling needed!")

#     # Initialize tokenizer
#     tokenizer = AutoTokenizer.from_pretrained("answerdotai/ModernBERT-base")

#     # STEP 1: Split into train (70%) + validation (10%) + test (20%) - OPTIMIZED SPLIT
#     print("\nðŸ”„ STEP 1: Creating optimized train/validation/test split")
#     print("-"*60)

#     # First split: separate test set (20% - smaller for more training data)
#     train_val_texts, test_texts, train_val_job_labels, test_job_labels, train_val_gender_labels, test_gender_labels = train_test_split(
#         texts, job_labels, gender_labels, test_size=0.2, random_state=42, stratify=job_labels
#     )

#     # Second split: separate validation from training (10% of original, ~12.5% of remaining)
#     train_texts, val_texts, train_job_labels, val_job_labels, train_gender_labels, val_gender_labels = train_test_split(
#         train_val_texts, train_val_job_labels, train_val_gender_labels, test_size=0.125, random_state=42, stratify=train_val_job_labels
#     )

#     print(f"Train: {len(train_texts)} samples ({len(train_texts)/len(texts)*100:.1f}%)")
#     print(f"Validation: {len(val_texts)} samples ({len(val_texts)/len(texts)*100:.1f}%)")
#     print(f"Test: {len(test_texts)} samples ({len(test_texts)/len(texts)*100:.1f}%)")
#     print(f"Test ICT count: {sum(test_job_labels)} samples")
#     print(f"Validation ICT count: {sum(val_job_labels)} samples")
#     print(f"Test ICT ratio: {sum(test_job_labels)/len(test_job_labels)*100:.1f}%")
#     print(f"Validation ICT ratio: {sum(val_job_labels)/len(val_job_labels)*100:.1f}%")

#     # STEP 2: Load best pre-trained model from previous transfer learning
#     print("\nðŸ”„ STEP 2: Loading best pre-trained model from previous transfer learning")
#     print("-"*60)

#     try:
#         base_pretrained_model, best_fold_num, best_pretrained_f1 = load_best_pretrained_model(device)
#         print(f"âœ… Successfully loaded pre-trained model from fold {best_fold_num} with F1: {best_pretrained_f1:.4f}")
#         print(f"âœ… This model will be used as starting point for ALL 3-fold CV runs")
#     except FileNotFoundError as e:
#         print(f"âŒ ERROR: Required pre-trained model not found!")
#         print(f"âŒ Please run initial cross-validation first to generate the best model file.")
#         print(f"âŒ {str(e)}")
#         raise  # Re-raise the exception to abort execution

#     # STEP 3: Run 3-fold cross-validation on train+val data (USING PRE-TRAINED MODEL)
#     print("\nðŸ”„ STEP 3: Running 3-fold cross-validation on train+val data")
#     print("-"*60)

#     # Prepare data for k-fold
#     skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
#     fold_results = []
#     best_fold = None
#     best_overall_f1 = 0.0  # Track overall F1, not just ICT F1
#     best_model_state = None

#     for fold, (train_idx, val_idx) in enumerate(skf.split(np.array(train_texts), np.array(train_job_labels))):
#         print(f"\nâ­ FOLD {fold+1}/3 â­")
#         print("-"*40)

#         # Split data for this fold
#         fold_train_texts = [train_texts[i] for i in train_idx]
#         fold_train_job_labels = [train_job_labels[i] for i in train_idx]
#         fold_train_gender_labels = [train_gender_labels[i] for i in train_idx]

#         fold_val_texts = [train_texts[i] for i in val_idx]
#         fold_val_job_labels = [train_job_labels[i] for i in val_idx]
#         fold_val_gender_labels = [train_gender_labels[i] for i in val_idx]

#         # UPDATED: Calculate simple balanced class weights (no extreme weighting)
#         class_weights = calculate_class_weights(fold_train_job_labels, weight_adjustment=1.1)  # Slight adjustment only
#         class_weights = class_weights.to(device)

#         # Create datasets and dataloaders with LARGER BATCH SIZES
#         train_dataset = ProfessionalDataset(fold_train_texts, fold_train_job_labels, fold_train_gender_labels, tokenizer)
#         val_dataset = ProfessionalDataset(fold_val_texts, fold_val_job_labels, fold_val_gender_labels, tokenizer)

#         # UPDATED: Larger batch size for stability
#         batch_size = 32  # Increased from 16
#         train_loader = create_dataloader(train_dataset, batch_size=batch_size, shuffle=True)
#         val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

#         print(f"  Fold {fold+1} - Train: {len(train_dataset)}, Val: {len(val_dataset)}")
#         print(f"  Fold {fold+1} - Val ICT ratio: {sum(fold_val_job_labels)/len(fold_val_job_labels):.3f}")

#         # Initialize model for this fold using the pre-trained model
#         print(f"  ðŸ”„ Creating model for fold {fold+1} using pre-trained weights from fold {best_fold_num}")

#         # UPDATED: More conservative regularization for stability
#         config = {
#             "hidden_dropout_prob": 0.3,  # More conservative (was 0.4)
#             "attention_probs_dropout_prob": 0.2  # More conservative (was 0.3)
#         }

#         fold_model = AutoModelForSequenceClassification.from_pretrained(
#             "answerdotai/ModernBERT-base",
#             num_labels=2,
#             low_cpu_mem_usage=True,
#             config=config
#         ).to(device)

#         # Load the pre-trained weights from the best previous model
#         fold_model.load_state_dict(base_pretrained_model.state_dict())
#         print(f"  âœ… Pre-trained weights loaded with balanced regularization (dropout: 0.3/0.2)")

#         baseline_model = BaselineModel(fold_model).to(device)

#         # UPDATED: Train with gradient accumulation and larger batches
#         results = train_baseline_model_balanced(baseline_model, train_loader, val_loader, device,
#                                               class_weights, epochs_frozen=2, epochs_unfrozen=5, accumulation_steps=2)

#         # Evaluate final model
#         test_metrics = evaluate_baseline_model(baseline_model, val_loader, device, print_metrics=False)

#         # Find optimal threshold
#         threshold_result = optimize_threshold(baseline_model, val_loader, device)
#         optimal_threshold = threshold_result['threshold']

#         # Final evaluation with optimal threshold
#         final_metrics = evaluate_baseline_model(baseline_model, val_loader, device,
#                                              threshold=optimal_threshold, print_metrics=False)

#         overall_f1 = np.mean(final_metrics['f1'])
#         ict_f1 = final_metrics['f1'][1]  # ICT F1 for reference

#         # Store fold results
#         fold_result = {
#             'fold': fold+1,
#             'accuracy': final_metrics['accuracy'],
#             'overall_f1': overall_f1,
#             'ict_f1': ict_f1,
#             'threshold': optimal_threshold,
#             'class_f1': {
#                 'non_ict': final_metrics['f1'][0],
#                 'ict': final_metrics['f1'][1]
#             }
#         }
#         fold_results.append(fold_result)

#         print(f"  Fold {fold+1} Results: Acc={final_metrics['accuracy']:.4f}, Overall F1={overall_f1:.4f}, ICT F1={ict_f1:.4f}")

#         # UPDATED: Track best fold based on overall F1 (balanced approach)
#         if overall_f1 > best_overall_f1:
#             best_overall_f1 = overall_f1
#             best_fold = fold+1
#             best_model_state = baseline_model.bert_classifier.state_dict().copy()
#             print(f"  âœ… New best fold based on overall F1!")

#     # Print CV summary
#     print(f"\nðŸ“‹ 3-FOLD CV SUMMARY (Balanced Classification):")
#     print("-"*50)
#     for result in fold_results:
#         print(f"Fold {result['fold']}: Acc={result['accuracy']:.4f}, Overall F1={result['overall_f1']:.4f}, ICT F1={result['ict_f1']:.4f}")

#     avg_overall_f1 = np.mean([r['overall_f1'] for r in fold_results])
#     avg_ict_f1 = np.mean([r['ict_f1'] for r in fold_results])
#     print(f"Average Overall F1: {avg_overall_f1:.4f}")
#     print(f"Average ICT F1: {avg_ict_f1:.4f}")
#     print(f"ðŸ† Best Fold: {best_fold} (Overall F1: {best_overall_f1:.4f})")
#     print(f"ðŸ“ˆ Transfer Learning: Using pre-trained model from fold {best_fold_num} (F1: {best_pretrained_f1:.4f}) as starting point")

#     # STEP 3.5: Lambda Parameter Optimization
#     print(f"\nðŸ”„ STEP 3.5: Lambda Parameter Optimization")
#     print("-"*60)

#     # Create subset of training data for lambda optimization (for efficiency)
#     lambda_train_size = min(len(train_texts), 400)  # Use subset for speed
#     lambda_val_size = min(len(val_texts), 100)

#     # Create datasets for lambda optimization
#     lambda_train_texts = train_texts[:lambda_train_size]
#     lambda_train_job_labels = train_job_labels[:lambda_train_size]
#     lambda_train_gender_labels = train_gender_labels[:lambda_train_size]

#     lambda_val_texts = val_texts[:lambda_val_size]
#     lambda_val_job_labels = val_job_labels[:lambda_val_size]
#     lambda_val_gender_labels = val_gender_labels[:lambda_val_size]

#     # UPDATED: Calculate balanced class weights for lambda optimization
#     lambda_class_weights = calculate_class_weights(lambda_train_job_labels, weight_adjustment=1.1)
#     lambda_class_weights = lambda_class_weights.to(device)

#     # Create datasets
#     lambda_train_dataset = ProfessionalDataset(lambda_train_texts, lambda_train_job_labels,
#                                              lambda_train_gender_labels, tokenizer)
#     lambda_val_dataset = ProfessionalDataset(lambda_val_texts, lambda_val_job_labels,
#                                            lambda_val_gender_labels, tokenizer)

#     print(f"Lambda optimization: {len(lambda_train_dataset)} train, {len(lambda_val_dataset)} val samples")

#     # Run lambda optimization
#     optimal_lambda, lambda_results = optimize_lambda_parameter(
#         base_model_state=best_model_state,  # Use best CV model weights
#         train_dataset=lambda_train_dataset,
#         val_dataset=lambda_val_dataset,
#         device=device,
#         class_weights=lambda_class_weights,
#         tokenizer=tokenizer,
#         lambda_values=[0.1, 0.5, 1.0, 1.5, 2.0, 5.0],  # Test range
#         max_epochs=3
#     )

#     # Plot results
#     plot_lambda_optimization_results(lambda_results)

#     print(f"âœ… Optimal lambda selected: {optimal_lambda}")

#     # STEP 4: Retrain best model on ALL train+val data
#     print(f"\nðŸ”„ STEP 4: Retraining best model (from fold {best_fold}) on ALL train+val data with optimal Î»={optimal_lambda}")
#     print("-"*60)

#     # UPDATED: Combine train + val for final training (use all available data)
#     all_train_texts = train_texts + val_texts
#     all_train_job_labels = train_job_labels + val_job_labels
#     all_train_gender_labels = train_gender_labels + val_gender_labels

#     # UPDATED: Calculate balanced class weights for full training data
#     final_class_weights = calculate_class_weights(all_train_job_labels, weight_adjustment=1.1)
#     final_class_weights = final_class_weights.to(device)

#     # Create full training dataset with LARGER BATCH SIZE
#     full_train_dataset = ProfessionalDataset(all_train_texts, all_train_job_labels, all_train_gender_labels, tokenizer)
#     full_train_loader = create_dataloader(full_train_dataset, batch_size=32, shuffle=True)  # Larger batch size

#     print(f"Full training set: {len(full_train_dataset)} samples (train + val combined)")
#     print(f"ðŸ“Š Using batch size 32 with gradient accumulation for stability")
#     print(f"âœ… Training on ALL available data (no validation holdout for final training)")

#     # Initialize final models using the best CV model weights
#     print(f"ðŸ”„ Initializing final models using best CV weights from fold {best_fold}")

#     # Use balanced dropout for final models
#     config = {
#         "hidden_dropout_prob": 0.3,  # More conservative
#         "attention_probs_dropout_prob": 0.2  # More conservative
#     }

#     final_base_model = AutoModelForSequenceClassification.from_pretrained(
#         "answerdotai/ModernBERT-base",
#         num_labels=2,
#         low_cpu_mem_usage=True,
#         config=config
#     ).to(device)

#     final_baseline_model = BaselineModel(final_base_model).to(device)
#     final_baseline_model.bert_classifier.load_state_dict(best_model_state)

#     # Create adversarial model using the same best weights
#     final_adv_base_model = AutoModelForSequenceClassification.from_pretrained(
#         "answerdotai/ModernBERT-base",
#         num_labels=2,
#         low_cpu_mem_usage=True,
#         config=config
#     ).to(device)
#     final_adv_base_model.load_state_dict(final_base_model.state_dict())
#     final_adversarial_model = AdversarialModel(final_adv_base_model, lambda_=optimal_lambda).to(device)

#     print(f"âœ… Models initialized with best CV fold {best_fold} weights (originally from pre-trained fold {best_fold_num})")
#     print(f"âœ… Using balanced regularization (dropout: 0.3/0.2) for stable training")

#     # UPDATED: Train final models on ALL data (no validation monitoring during final training)
#     print(f"\nðŸš€ Training final baseline model on ALL train+val data (10 epochs with early stopping)...")
#     baseline_results = train_final_baseline_model(final_baseline_model, full_train_loader, device,
#                                                  final_class_weights, max_epochs=10, accumulation_steps=2)

#     print(f"\nðŸš€ Training final adversarial model on ALL train+val data (10 epochs with early stopping)...")
#     adversarial_results = train_final_adversarial_model(final_adversarial_model, full_train_loader, device,
#                                                        final_class_weights, max_epochs=10, lambda_=optimal_lambda, accumulation_steps=2)

#     # STEP 5: Final evaluation on held-out test set
#     print(f"\nðŸ”„ STEP 5: Final evaluation on held-out test set")
#     print("="*60)

#     # Create test dataset with larger batch size
#     test_dataset = ProfessionalDataset(test_texts, test_job_labels, test_gender_labels, tokenizer)
#     test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

#     print(f"Test set: {len(test_dataset)} samples")
#     print(f"Test ICT count: {sum(test_job_labels)} samples")
#     print(f"Test ICT ratio: {sum(test_job_labels)/len(test_job_labels)*100:.1f}%")

#     # Find optimal thresholds using test set (since we used all train+val for training)
#     print(f"\nðŸ” Finding optimal thresholds using test set distribution...")
#     baseline_threshold_result = optimize_threshold(final_baseline_model, test_loader, device, is_adversarial=False)
#     baseline_threshold = baseline_threshold_result['threshold']

#     adversarial_threshold_result = optimize_threshold(final_adversarial_model, test_loader, device, is_adversarial=True)
#     adversarial_threshold = adversarial_threshold_result['threshold']

#     print(f"Baseline optimal threshold: {baseline_threshold:.4f}")
#     print(f"Adversarial optimal threshold: {adversarial_threshold:.4f}")

#     # Final test set evaluation
#     print(f"\nðŸ“Š FINAL TEST SET EVALUATION (Optimized Balanced Binary Classification)")
#     print("="*60)

#     baseline_test_metrics = evaluate_baseline_model(final_baseline_model, test_loader, device,
#                                                    threshold=baseline_threshold, dataset_name="Test (Baseline)")

#     adversarial_test_metrics = evaluate_adversarial_model(final_adversarial_model, test_loader, device,
#                                                         threshold=adversarial_threshold, dataset_name="Test (Adversarial)")

#     # Test gender predictability from baseline features
#     print(f"\nðŸ” Testing gender predictability from baseline features...")
#     final_baseline_model.eval()
#     all_features = []
#     all_gender_labels = []

#     with torch.no_grad():
#         for batch in test_loader:
#             input_ids = batch['input_ids'].to(device)
#             attention_mask = batch['attention_mask'].to(device)
#             gender_labels = batch['gender_labels']

#             outputs = final_baseline_model.bert_classifier.model(input_ids=input_ids, attention_mask=attention_mask)
#             pooled_output = outputs.last_hidden_state[:, 0, :]

#             all_features.append(pooled_output.cpu())
#             all_gender_labels.extend(gender_labels.numpy())

#     # Quick gender classifier training
#     all_features = torch.cat(all_features, dim=0)
#     gender_classifier = nn.Linear(all_features.shape[1], 2)
#     gender_optimizer = torch.optim.Adam(gender_classifier.parameters(), lr=1e-3)
#     gender_criterion = nn.CrossEntropyLoss()

#     gender_dataset = torch.utils.data.TensorDataset(all_features, torch.tensor(all_gender_labels, dtype=torch.long))
#     gender_loader = DataLoader(gender_dataset, batch_size=32, shuffle=True)

#     gender_classifier.train()
#     for epoch in range(10):
#         for features, gender_labels in gender_loader:
#             gender_optimizer.zero_grad()
#             gender_logits = gender_classifier(features)
#             loss = gender_criterion(gender_logits, gender_labels)
#             loss.backward()
#             gender_optimizer.step()

#     gender_classifier.eval()
#     with torch.no_grad():
#         gender_logits = gender_classifier(all_features)
#         gender_preds = torch.argmax(gender_logits, dim=1)
#         baseline_gender_acc = accuracy_score(all_gender_labels, gender_preds.numpy())

#     print(f"Gender predictability from baseline features: {baseline_gender_acc:.4f}")
#     print(f"(Closer to 0.5 means less gender bias)")

#     # Display confusion matrices
#     print(f"\nðŸ“Š Test Set Confusion Matrices:")
#     print(f"\nBaseline Model (Test Set):")
#     plot_confusion_matrix(baseline_test_metrics['confusion_matrix'], title='Baseline Model - Test Set (Optimized)')

#     print(f"\nAdversarial Model (Test Set):")
#     plot_confusion_matrix(adversarial_test_metrics['confusion_matrix'], title='Adversarial Model - Test Set (Optimized)')

#     # Final comparison table
#     print(f"\nðŸ“‹ FINAL TEST RESULTS COMPARISON (Optimized Balanced Binary Classification)")
#     print("-" * 115)
#     print(f"{'Model':<25} {'Overall F1':<12} {'ICT F1':<10} {'Job Acc':<10} {'Gender Acc':<12} {'Demo Parity':<12}")
#     print("-" * 115)
#     print(f"{'Original Pre-trained':<25} {best_pretrained_f1:<12.4f} {'N/A':<10} {'N/A':<10} {'N/A':<12} {'N/A':<12}")
#     print(f"{'CV Best (Fold ' + str(best_fold) + ')':<25} {best_overall_f1:<12.4f} {avg_ict_f1:<10.4f} {'N/A':<10} {'N/A':<12} {'N/A':<12}")
#     print(f"{'Baseline (Test)':<25} {np.mean(baseline_test_metrics['f1']):<12.4f} {baseline_test_metrics['f1'][1]:<10.4f} {baseline_test_metrics['accuracy']:<10.4f} {baseline_gender_acc:<12.4f} {baseline_test_metrics['demo_parity_diff']:<12.4f}")
#     print(f"{'Adversarial (Test)':<25} {np.mean(adversarial_test_metrics['f1']):<12.4f} {adversarial_test_metrics['f1'][1]:<10.4f} {adversarial_test_metrics['job_accuracy']:<10.4f} {adversarial_test_metrics['gender_accuracy']:<12.4f} {adversarial_test_metrics['demo_parity_diff']:<12.4f}")
#     print("-" * 115)
#     print(f"ðŸ“ˆ Transfer Learning Path: Fold {best_fold_num} â†’ CV Fold {best_fold} â†’ Final Test")
#     print(f"âš–ï¸ Class Weights Used: non-ICT={final_class_weights[0]:.3f}, ICT={final_class_weights[1]:.3f}")
#     print(f"ðŸŽ¯ Test Set Size: {len(test_dataset)} samples ({sum(test_job_labels)} ICT)")
#     print(f"ðŸ”§ Pipeline: Optimized with batch size 32, gradient accumulation, conservative LR")
#     print(f"ðŸ“Š Approach: Balanced binary classification with adversarial debiasing")
#     print(f"ðŸŽ¯ Expected: Both classes >75% F1 with optimized training")

#     # UPDATED: Success criteria for balanced classification with higher expectations
#     print(f"\nâœ… SUCCESS CRITERIA CHECK (Balanced Binary Classification - Improved Pipeline):")
#     job_f1_drop = np.mean(baseline_test_metrics['f1']) - np.mean(adversarial_test_metrics['f1'])
#     gender_acc_drop = baseline_gender_acc - adversarial_test_metrics['gender_accuracy']

#     print(f"1. Job F1 drop < 0.05: {job_f1_drop:.4f} {'âœ…' if job_f1_drop < 0.05 else 'âŒ'}")
#     print(f"2. Gender Acc drop > 0.20: {gender_acc_drop:.4f} {'âœ…' if gender_acc_drop > 0.20 else 'âŒ'}")
#     print(f"3. Adversarial Gender Acc < 0.60: {adversarial_test_metrics['gender_accuracy']:.4f} {'âœ…' if adversarial_test_metrics['gender_accuracy'] < 0.60 else 'âŒ'}")
#     print(f"4. ICT F1 > 0.75: {baseline_test_metrics['f1'][1]:.4f} {'âœ…' if baseline_test_metrics['f1'][1] > 0.75 else 'âŒ'}")  # UPDATED: Higher expectation
#     print(f"5. Overall F1 > 0.75: {np.mean(baseline_test_metrics['f1']):.4f} {'âœ…' if np.mean(baseline_test_metrics['f1']) > 0.75 else 'âŒ'}")  # NEW: Overall performance

#     if (job_f1_drop < 0.05 and gender_acc_drop > 0.20 and
#         adversarial_test_metrics['gender_accuracy'] < 0.60 and
#         baseline_test_metrics['f1'][1] > 0.75 and
#         np.mean(baseline_test_metrics['f1']) > 0.75):
#         print(f"\nðŸŽ‰ SUCCESS! Excellent balanced classification with effective debiasing!")
#     else:
#         print(f"\nâš ï¸ Room for improvement. With optimized pipeline, both classes should achieve >75% F1.")

#     return {
#         'cv_results': fold_results,
#         'best_fold': best_fold,
#         'best_overall_f1': best_overall_f1,
#         'test_baseline': baseline_test_metrics,
#         'test_adversarial': adversarial_test_metrics,
#         'baseline_gender_acc': baseline_gender_acc,
#         'class_weights_used': final_class_weights.cpu().numpy(),
#         'improvements_applied': {
#             'optimized_data_split': '70% train, 10% val, 20% test (more training data)',
#             'final_training_data': 'Train + Val combined for final training (no validation holdout)',
#             'larger_batch_sizes': 'Batch size 32 with gradient accumulation (2 steps)',
#             'conservative_regularization': 'Dropout 0.3/0.2 for stability',
#             'conservative_learning_rates': 'Head: 3e-4, Full: 8e-6 (prevent overfitting)',
#             'higher_weight_decay': '0.02 for better generalization',
#             'gradient_accumulation': '2 steps for effective batch size 64',
#             'extended_training': '10 epochs with early stopping for final models',
#             'no_frozen_layers_final': 'Final training uses full model (no frozen backbone)',
#             'balanced_classification': 'Standard binary classification (no minority handling)',
#             'transfer_learning_preserved': 'Best CV model â†’ Final training',
#             'expected_performance': 'Both ICT and non-ICT F1 should be >75% with optimized pipeline'
#         }
#     }

# # ====================================================================================
# # UPDATED TRAINING FUNCTIONS - FINAL TRAINING ON ALL DATA
# # ====================================================================================

def train_final_baseline_model(model, train_loader, device, class_weights, max_epochs=10, accumulation_steps=2):
    """Train baseline model on ALL available data (train+val combined) with early stopping"""
    print("\nðŸš€ Final Baseline Training - Full Model on ALL Data")
    print("="*70)

    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1) #label_smoothing=0.1?
    results = []

    # Early stopping based on training loss stabilization
    best_loss = float('inf')
    patience = 5
    patience_counter = 0

    print(f"Using balanced class weights: non-ICT={class_weights[0]:.4f}, ICT={class_weights[1]:.4f}")
    print(f"ðŸŽ¯ Early stopping based on training loss stabilization")
    print(f"ðŸ“Š Max epochs: {max_epochs}, Gradient accumulation: {accumulation_steps}")
    print(f"ðŸ”„ Training on ALL available data (no validation holdout)")

    # UPDATED: Train full model directly (no frozen layers)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6, weight_decay=0.1, eps=1e-8)  # Very conservative LR

    for epoch in range(max_epochs):
        model.train()
        total_loss = 0
        train_job_preds = []
        train_job_labels = []

        print(f"\nEpoch {epoch+1}/{max_epochs} (Full model training)")
        optimizer.zero_grad()

        for batch_idx, batch in enumerate(train_loader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['job_labels'].to(device)

            logits = model(input_ids, attention_mask)
            loss = criterion(logits, labels)

            # Gradient accumulation
            loss = loss / accumulation_steps
            loss.backward()

            total_loss += loss.item() * accumulation_steps

            # Update gradients every accumulation_steps
            if ((batch_idx + 1) % accumulation_steps == 0) or (batch_idx + 1 == len(train_loader)):
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                optimizer.zero_grad()

            preds = torch.argmax(logits, dim=1)
            train_job_preds.extend(preds.cpu().numpy())
            train_job_labels.extend(labels.cpu().numpy())

            if (batch_idx+1) % 20 == 0:
                print(f"  Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item() * accumulation_steps:.4f}")

        # Calculate training metrics
        avg_loss = total_loss / len(train_loader)
        train_acc = accuracy_score(train_job_labels, train_job_preds)
        train_f1 = f1_score(train_job_labels, train_job_preds, average='weighted')
        train_ict_f1 = f1_score(train_job_labels, train_job_preds, average=None)[1]

        print(f"  Epoch {epoch+1} Summary:")
        print(f"    Train - Loss: {avg_loss:.4f}, Acc: {train_acc:.4f}, Overall F1: {train_f1:.4f}, ICT F1: {train_ict_f1:.4f}")

        # Early stopping check based on loss improvement
        if avg_loss < best_loss:
            best_loss = avg_loss
            patience_counter = 0
            print(f"    âœ… New best loss: {best_loss:.4f}")
        else:
            patience_counter += 1
            print(f"    ðŸ“ˆ Loss: {avg_loss:.4f} (best: {best_loss:.4f}, patience: {patience_counter}/{patience})")
            if patience_counter >= patience:
                print(f"    ðŸ›‘ Early stopping: Loss not improving for {patience} epochs")
                break

        results.append({
            'epoch': epoch+1,
            'loss': avg_loss,
            'train_acc': train_acc,
            'train_f1': train_f1,
            'train_ict_f1': train_ict_f1
        })

    print(f"\nâœ… Final baseline training complete. Best loss: {best_loss:.4f}")
    return results

# def train_final_adversarial_model(model, train_loader, device, class_weights, max_epochs=6, lambda_=1.0, accumulation_steps=2):
#     """Train adversarial model on ALL available data (train+val combined) with early stopping"""
#     print("\nðŸš€ Final Adversarial Training - Full Model on ALL Data")
#     print("="*70)

#     job_criterion = nn.CrossEntropyLoss(weight=class_weights)
#     gender_criterion = nn.CrossEntropyLoss()
#     results = []

#     # Early stopping based on combined loss stabilization
#     best_combined_loss = float('inf')
#     patience = 3
#     patience_counter = 0

#     print(f"Using balanced class weights: non-ICT={class_weights[0]:.4f}, ICT={class_weights[1]:.4f}")
#     print(f"ðŸŽ¯ Early stopping based on combined loss stabilization")
#     print(f"ðŸŽ¯ Lambda parameter: {lambda_}")
#     print(f"ðŸ“Š Max epochs: {max_epochs}, Gradient accumulation: {accumulation_steps}")
#     print(f"ðŸ”„ Training on ALL available data (no validation holdout)")

#     # UPDATED: Train full model directly (no frozen layers)
#     optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6, weight_decay=0.02, eps=1e-8)  # Very conservative LR

#     for epoch in range(max_epochs):
#         model.train()
#         total_job_loss = 0
#         total_gender_loss = 0
#         total_combined_loss = 0
#         train_job_preds = []
#         train_job_labels = []

#         print(f"\nEpoch {epoch+1}/{max_epochs} (Full model training)")
#         optimizer.zero_grad()

#         for batch_idx, batch in enumerate(train_loader):
#             input_ids = batch['input_ids'].to(device)
#             attention_mask = batch['attention_mask'].to(device)
#             job_labels = batch['job_labels'].to(device)
#             gender_labels = batch['gender_labels'].to(device)

#             job_logits, gender_logits = model(input_ids, attention_mask)

#             job_loss = job_criterion(job_logits, job_labels)
#             gender_loss = gender_criterion(gender_logits, gender_labels)
#             combined_loss = job_loss + lambda_ * gender_loss

#             # Gradient accumulation
#             combined_loss = combined_loss / accumulation_steps
#             combined_loss.backward()

#             total_job_loss += job_loss.item()
#             total_gender_loss += gender_loss.item()
#             total_combined_loss += combined_loss.item() * accumulation_steps

#             # Update gradients every accumulation_steps
#             if ((batch_idx + 1) % accumulation_steps == 0) or (batch_idx + 1 == len(train_loader)):
#                 torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
#                 optimizer.step()
#                 optimizer.zero_grad()

#             job_preds = torch.argmax(job_logits, dim=1)
#             train_job_preds.extend(job_preds.cpu().numpy())
#             train_job_labels.extend(job_labels.cpu().numpy())

#             if (batch_idx+1) % 20 == 0:
#                 print(f"  Batch {batch_idx+1}/{len(train_loader)}, Job Loss: {job_loss.item():.4f}, Gender Loss: {gender_loss.item():.4f}")

#         # Calculate training metrics
#         avg_job_loss = total_job_loss / len(train_loader)
#         avg_gender_loss = total_gender_loss / len(train_loader)
#         avg_combined_loss = total_combined_loss / len(train_loader)
#         train_job_acc = accuracy_score(train_job_labels, train_job_preds)
#         train_job_f1 = f1_score(train_job_labels, train_job_preds, average='weighted')
#         train_ict_f1 = f1_score(train_job_labels, train_job_preds, average=None)[1]

#         print(f"  Epoch {epoch+1} Summary:")
#         print(f"    Train - Job Loss: {avg_job_loss:.4f}, Gender Loss: {avg_gender_loss:.4f}, Combined: {avg_combined_loss:.4f}")
#         print(f"    Train - Job Acc: {train_job_acc:.4f}, Overall F1: {train_job_f1:.4f}, ICT F1: {train_ict_f1:.4f}")

#         # Early stopping check based on combined loss improvement
#         if avg_combined_loss < best_combined_loss:
#             best_combined_loss = avg_combined_loss
#             patience_counter = 0
#             print(f"    âœ… New best combined loss: {best_combined_loss:.4f}")
#         else:
#             patience_counter += 1
#             print(f"    ðŸ“ˆ Combined Loss: {avg_combined_loss:.4f} (best: {best_combined_loss:.4f}, patience: {patience_counter}/{patience})")
#             if patience_counter >= patience:
#                 print(f"    ðŸ›‘ Early stopping: Combined loss not improving for {patience} epochs")
#                 break

#         results.append({
#             'epoch': epoch+1,
#             'job_loss': avg_job_loss,
#             'gender_loss': avg_gender_loss,
#             'combined_loss': avg_combined_loss,
#             'train_job_acc': train_job_acc,
#             'train_job_f1': train_job_f1,
#             'train_ict_f1': train_ict_f1
#         })

#     print(f"\nâœ… Final adversarial training complete. Best combined loss: {best_combined_loss:.4f}")
#     return results

# # ====================================================================================
# # UPDATED TRAINING FUNCTIONS - BALANCED BINARY CLASSIFICATION (FOR CV)
# # ====================================================================================

def train_baseline_model_balanced(model, train_loader, val_loader, device, class_weights,
                                epochs_frozen=2, epochs_unfrozen=3, accumulation_steps=2):
    """Train baseline model with standard balanced binary classification approach + gradient accumulation"""
    print("\nðŸš€ Training Baseline Model - Balanced Binary Classification")
    print("="*70)

    # Use standard CrossEntropyLoss (Focal Loss optional with mild gamma)
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    results = []

    # Early stopping based on overall validation F1 (not ICT-specific)
    best_val_f1 = 0
    patience = 3
    patience_counter = 0

    print(f"Using balanced class weights: non-ICT={class_weights[0]:.4f}, ICT={class_weights[1]:.4f}")
    print(f"ðŸŽ¯ Early stopping based on overall validation F1 (balanced approach)")
    print(f"ðŸ“Š Gradient accumulation steps: {accumulation_steps}")

    # Phase 1: Freeze backbone, train classifier head
    print(f"\n--- Phase 1: Frozen backbone training ({epochs_frozen} epochs) ---")
    for param in model.bert_classifier.model.parameters():
        param.requires_grad = False
    # UPDATED: More conservative learning rate and weight decay
    optimizer = torch.optim.AdamW(model.bert_classifier.classifier.parameters(), lr=3e-4, weight_decay=0.02, eps=1e-8)

    for epoch in range(epochs_frozen):
        model.train()
        total_loss = 0
        train_job_preds = []
        train_job_labels = []

        print(f"\nEpoch {epoch+1}/{epochs_frozen} (Frozen phase)")
        optimizer.zero_grad()  # Initialize gradients

        for batch_idx, batch in enumerate(train_loader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['job_labels'].to(device)

            logits = model(input_ids, attention_mask)
            loss = criterion(logits, labels)

            # UPDATED: Gradient accumulation
            loss = loss / accumulation_steps
            loss.backward()

            total_loss += loss.item() * accumulation_steps  # Scale back for logging

            # Update gradients every accumulation_steps
            if ((batch_idx + 1) % accumulation_steps == 0) or (batch_idx + 1 == len(train_loader)):
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                optimizer.zero_grad()

            preds = torch.argmax(logits, dim=1)
            train_job_preds.extend(preds.cpu().numpy())
            train_job_labels.extend(labels.cpu().numpy())

            if (batch_idx+1) % 15 == 0:  # Less frequent logging for larger batches
                print(f"  Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item() * accumulation_steps:.4f}")

        # Calculate training metrics
        train_acc = accuracy_score(train_job_labels, train_job_preds)
        train_f1 = f1_score(train_job_labels, train_job_preds, average='weighted')
        train_ict_f1 = f1_score(train_job_labels, train_job_preds, average=None)[1]

        # Evaluate on validation set
        val_metrics = evaluate_baseline_model(model, val_loader, device, print_metrics=False)
        val_acc = val_metrics['accuracy']
        val_f1 = np.mean(val_metrics['f1'])
        val_ict_f1 = val_metrics['f1'][1]

        print(f"  Epoch {epoch+1} Summary:")
        print(f"    Train - Loss: {total_loss/len(train_loader):.4f}, Acc: {train_acc:.4f}, Overall F1: {train_f1:.4f}, ICT F1: {train_ict_f1:.4f}")
        print(f"    Val   - Acc: {val_acc:.4f}, Overall F1: {val_f1:.4f}, ICT F1: {val_ict_f1:.4f}")

        # Early stopping check based on overall F1
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            patience_counter = 0
            print(f"    âœ… New best overall F1: {best_val_f1:.4f}")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"    ðŸ›‘ Early stopping triggered after {epoch+1} epochs (Overall F1 not improving).")
                break

        results.append({
            'epoch': epoch+1,
            'phase': 'frozen',
            'loss': total_loss/len(train_loader),
            'train_acc': train_acc,
            'train_f1': train_f1,
            'train_ict_f1': train_ict_f1,
            'val_acc': val_acc,
            'val_f1': val_f1,
            'val_ict_f1': val_ict_f1
        })

    # Phase 2: Full fine-tuning
    print(f"\n--- Phase 2: Full fine-tuning ({epochs_unfrozen} epochs) ---")
    for param in model.bert_classifier.model.parameters():
        param.requires_grad = True
    # UPDATED: More conservative learning rate and weight decay for full model
    optimizer = torch.optim.AdamW(model.parameters(), lr=8e-6, weight_decay=0.02, eps=1e-8)

    for epoch in range(epochs_unfrozen):
        model.train()
        total_loss = 0
        train_job_preds = []
        train_job_labels = []

        print(f"\nEpoch {epoch+1}/{epochs_unfrozen} (Full fine-tuning)")
        optimizer.zero_grad()  # Initialize gradients

        for batch_idx, batch in enumerate(train_loader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['job_labels'].to(device)

            logits = model(input_ids, attention_mask)
            loss = criterion(logits, labels)

            # UPDATED: Gradient accumulation
            loss = loss / accumulation_steps
            loss.backward()

            total_loss += loss.item() * accumulation_steps  # Scale back for logging

            # Update gradients every accumulation_steps
            if ((batch_idx + 1) % accumulation_steps == 0) or (batch_idx + 1 == len(train_loader)):
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                optimizer.zero_grad()

            preds = torch.argmax(logits, dim=1)
            train_job_preds.extend(preds.cpu().numpy())
            train_job_labels.extend(labels.cpu().numpy())

            if (batch_idx+1) % 15 == 0:  # Less frequent logging for larger batches
                print(f"  Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item() * accumulation_steps:.4f}")

        # Calculate training metrics
        train_acc = accuracy_score(train_job_labels, train_job_preds)
        train_f1 = f1_score(train_job_labels, train_job_preds, average='weighted')
        train_ict_f1 = f1_score(train_job_labels, train_job_preds, average=None)[1]

        # Evaluate on validation set
        val_metrics = evaluate_baseline_model(model, val_loader, device, print_metrics=False)
        val_acc = val_metrics['accuracy']
        val_f1 = np.mean(val_metrics['f1'])
        val_ict_f1 = val_metrics['f1'][1]

        print(f"  Epoch {epoch+1} Summary:")
        print(f"    Train - Loss: {total_loss/len(train_loader):.4f}, Acc: {train_acc:.4f}, Overall F1: {train_f1:.4f}, ICT F1: {train_ict_f1:.4f}")
        print(f"    Val   - Acc: {val_acc:.4f}, Overall F1: {val_f1:.4f}, ICT F1: {val_ict_f1:.4f}")

        # Early stopping check based on overall F1
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            patience_counter = 0
            print(f"    âœ… New best overall F1: {best_val_f1:.4f}")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"    ðŸ›‘ Early stopping triggered after {epoch+1} epochs (Overall F1 not improving).")
                break

        results.append({
            'epoch': epoch+1,
            'phase': 'unfrozen',
            'loss': total_loss/len(train_loader),
            'train_acc': train_acc,
            'train_f1': train_f1,
            'train_ict_f1': train_ict_f1,
            'val_acc': val_acc,
            'val_f1': val_f1,
            'val_ict_f1': val_ict_f1
        })

    print(f"\nâœ… Balanced training complete. Final best overall F1: {best_val_f1:.4f}")
    return results

# def train_adversarial_model_balanced(model, train_loader, val_loader, device, class_weights,
#                                    epochs_frozen=2, epochs_unfrozen=3, lambda_=1.0, accumulation_steps=2):
#     """Train adversarial model with standard balanced binary classification approach + gradient accumulation"""
#     print("\nðŸš€ Training Adversarial Model - Balanced Binary Classification")
#     print("="*70)

#     # Use standard CrossEntropyLoss for both tasks
#     job_criterion = nn.CrossEntropyLoss(weight=class_weights)
#     gender_criterion = nn.CrossEntropyLoss()
#     results = []

#     # Early stopping based on overall validation F1 (not ICT-specific)
#     best_val_f1 = 0
#     patience = 3
#     patience_counter = 0

#     print(f"Using balanced class weights: non-ICT={class_weights[0]:.4f}, ICT={class_weights[1]:.4f}")
#     print(f"ðŸŽ¯ Early stopping based on overall validation F1 (balanced approach)")
#     print(f"ðŸŽ¯ Lambda parameter: {lambda_}")
#     print(f"ðŸ“Š Gradient accumulation steps: {accumulation_steps}")

#     # Phase 1: Freeze backbone, train classifier heads
#     print(f"\n--- Phase 1: Frozen backbone training ({epochs_frozen} epochs) ---")
#     for param in model.bert.parameters():
#         param.requires_grad = False
#     # UPDATED: More conservative learning rate and weight decay
#     optimizer = torch.optim.AdamW(
#         list(model.job_classifier.parameters()) + list(model.gender_classifier.parameters()),
#         lr=3e-4, weight_decay=0.02, eps=1e-8
#     )

#     for epoch in range(epochs_frozen):
#         model.train()
#         total_job_loss = 0
#         total_gender_loss = 0
#         train_job_preds = []
#         train_job_labels = []

#         print(f"\nEpoch {epoch+1}/{epochs_frozen} (Frozen phase)")
#         optimizer.zero_grad()  # Initialize gradients

#         for batch_idx, batch in enumerate(train_loader):
#             input_ids = batch['input_ids'].to(device)
#             attention_mask = batch['attention_mask'].to(device)
#             job_labels = batch['job_labels'].to(device)
#             gender_labels = batch['gender_labels'].to(device)

#             job_logits, gender_logits = model(input_ids, attention_mask)

#             job_loss = job_criterion(job_logits, job_labels)
#             gender_loss = gender_criterion(gender_logits, gender_labels)
#             total_batch_loss = job_loss + lambda_ * gender_loss

#             # UPDATED: Gradient accumulation
#             total_batch_loss = total_batch_loss / accumulation_steps
#             total_batch_loss.backward()

#             total_job_loss += job_loss.item()
#             total_gender_loss += gender_loss.item()

#             # Update gradients every accumulation_steps
#             if ((batch_idx + 1) % accumulation_steps == 0) or (batch_idx + 1 == len(train_loader)):
#                 torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
#                 optimizer.step()
#                 optimizer.zero_grad()

#             job_preds = torch.argmax(job_logits, dim=1)
#             train_job_preds.extend(job_preds.cpu().numpy())
#             train_job_labels.extend(job_labels.cpu().numpy())

#             if (batch_idx+1) % 15 == 0:  # Less frequent logging for larger batches
#                 print(f"  Batch {batch_idx+1}/{len(train_loader)}, Job Loss: {job_loss.item():.4f}, Gender Loss: {gender_loss.item():.4f}")

#         # Calculate training metrics
#         train_job_acc = accuracy_score(train_job_labels, train_job_preds)
#         train_job_f1 = f1_score(train_job_labels, train_job_preds, average='weighted')
#         train_ict_f1 = f1_score(train_job_labels, train_job_preds, average=None)[1]

#         # Evaluate on validation set
#         val_metrics = evaluate_adversarial_model(model, val_loader, device, print_metrics=False)
#         val_job_acc = val_metrics['job_accuracy']
#         val_job_f1 = np.mean(val_metrics['f1'])
#         val_ict_f1 = val_metrics['f1'][1]

#         print(f"  Epoch {epoch+1} Summary:")
#         print(f"    Train - Job Loss: {total_job_loss/len(train_loader):.4f}, Job Acc: {train_job_acc:.4f}, Overall F1: {train_job_f1:.4f}, ICT F1: {train_ict_f1:.4f}")
#         print(f"    Val   - Job Acc: {val_job_acc:.4f}, Overall F1: {val_job_f1:.4f}, ICT F1: {val_ict_f1:.4f}")

#         # Early stopping check based on overall F1
#         if val_job_f1 > best_val_f1:
#             best_val_f1 = val_job_f1
#             patience_counter = 0
#             print(f"    âœ… New best overall F1: {best_val_f1:.4f}")
#         else:
#             patience_counter += 1
#             if patience_counter >= patience:
#                 print(f"    ðŸ›‘ Early stopping triggered after {epoch+1} epochs (Overall F1 not improving).")
#                 break

#         results.append({
#             'epoch': epoch+1,
#             'phase': 'frozen',
#             'job_loss': total_job_loss/len(train_loader),
#             'gender_loss': total_gender_loss/len(train_loader),
#             'train_job_acc': train_job_acc,
#             'train_job_f1': train_job_f1,
#             'train_ict_f1': train_ict_f1,
#             'val_job_acc': val_job_acc,
#             'val_job_f1': val_job_f1,
#             'val_ict_f1': val_ict_f1
#         })

#     # Phase 2: Full fine-tuning
#     print(f"\n--- Phase 2: Full fine-tuning ({epochs_unfrozen} epochs) ---")
#     for param in model.bert.parameters():
#         param.requires_grad = True
#     # UPDATED: More conservative learning rate and weight decay for full model
#     optimizer = torch.optim.AdamW(model.parameters(), lr=8e-6, weight_decay=0.02, eps=1e-8)

#     for epoch in range(epochs_unfrozen):
#         model.train()
#         total_job_loss = 0
#         total_gender_loss = 0
#         train_job_preds = []
#         train_job_labels = []

#         print(f"\nEpoch {epoch+1}/{epochs_unfrozen} (Full fine-tuning)")
#         optimizer.zero_grad()  # Initialize gradients

#         for batch_idx, batch in enumerate(train_loader):
#             input_ids = batch['input_ids'].to(device)
#             attention_mask = batch['attention_mask'].to(device)
#             job_labels = batch['job_labels'].to(device)
#             gender_labels = batch['gender_labels'].to(device)

#             job_logits, gender_logits = model(input_ids, attention_mask)

#             job_loss = job_criterion(job_logits, job_labels)
#             gender_loss = gender_criterion(gender_logits, gender_labels)
#             total_batch_loss = job_loss + lambda_ * gender_loss

#             # UPDATED: Gradient accumulation
#             total_batch_loss = total_batch_loss / accumulation_steps
#             total_batch_loss.backward()

#             total_job_loss += job_loss.item()
#             total_gender_loss += gender_loss.item()

#             # Update gradients every accumulation_steps
#             if ((batch_idx + 1) % accumulation_steps == 0) or (batch_idx + 1 == len(train_loader)):
#                 torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
#                 optimizer.step()
#                 optimizer.zero_grad()

#             job_preds = torch.argmax(job_logits, dim=1)
#             train_job_preds.extend(job_preds.cpu().numpy())
#             train_job_labels.extend(job_labels.cpu().numpy())

#             if (batch_idx+1) % 15 == 0:  # Less frequent logging for larger batches
#                 print(f"  Batch {batch_idx+1}/{len(train_loader)}, Job Loss: {job_loss.item():.4f}, Gender Loss: {gender_loss.item():.4f}")

#         # Calculate training metrics
#         train_job_acc = accuracy_score(train_job_labels, train_job_preds)
#         train_job_f1 = f1_score(train_job_labels, train_job_preds, average='weighted')
#         train_ict_f1 = f1_score(train_job_labels, train_job_preds, average=None)[1]

#         # Evaluate on validation set
#         val_metrics = evaluate_adversarial_model(model, val_loader, device, print_metrics=False)
#         val_job_acc = val_metrics['job_accuracy']
#         val_job_f1 = np.mean(val_metrics['f1'])
#         val_ict_f1 = val_metrics['f1'][1]

#         print(f"  Epoch {epoch+1} Summary:")
#         print(f"    Train - Job Loss: {total_job_loss/len(train_loader):.4f}, Job Acc: {train_job_acc:.4f}, Overall F1: {train_job_f1:.4f}, ICT F1: {train_ict_f1:.4f}")
#         print(f"    Val   - Job Acc: {val_job_acc:.4f}, Overall F1: {val_job_f1:.4f}, ICT F1: {val_ict_f1:.4f}")

#         # Early stopping check based on overall F1
#         if val_job_f1 > best_val_f1:
#             best_val_f1 = val_job_f1
#             patience_counter = 0
#             print(f"    âœ… New best overall F1: {best_val_f1:.4f}")
#         else:
#             patience_counter += 1
#             if patience_counter >= patience:
#                 print(f"    ðŸ›‘ Early stopping triggered after {epoch+1} epochs (Overall F1 not improving).")
#                 break

#         results.append({
#             'epoch': epoch+1,
#             'phase': 'unfrozen',
#             'job_loss': total_job_loss/len(train_loader),
#             'gender_loss': total_gender_loss/len(train_loader),
#             'train_job_acc': train_job_acc,
#             'train_job_f1': train_job_f1,
#             'train_ict_f1': train_ict_f1,
#             'val_job_acc': val_job_acc,
#             'val_job_f1': val_job_f1,
#             'val_ict_f1': val_ict_f1
#         })

#     print(f"\nâœ… Balanced adversarial training complete. Final best overall F1: {best_val_f1:.4f}")
#     return results

# # Run the main function
# if __name__ == "__main__":
#     results = main()

def optimize_probe_threshold(gender_classifier, X_test, y_test, device):
    """Find optimal threshold for gender probe classifier"""
    print("\nðŸ” Optimizing probe threshold for best accuracy...")

    gender_classifier.eval()

    # Get all predictions
    with torch.no_grad():
        test_logits = gender_classifier(X_test)
        gender_probs = F.softmax(test_logits, dim=1)[:, 1]  # Probability of class 1 (Male)

    gender_probs_np = gender_probs.cpu().numpy()
    y_test_np = y_test.cpu().numpy()

    # Try different thresholds
    thresholds = np.arange(0.2, 0.8, 0.02)  # More fine-grained
    results = []

    print("Testing thresholds:")
    for threshold in thresholds:
        preds = (gender_probs_np > threshold).astype(int)

        # Calculate metrics
        accuracy = accuracy_score(y_test_np, preds)
        f1 = f1_score(y_test_np, preds, average='weighted')
        precision = precision_score(y_test_np, preds, average='weighted')
        recall = recall_score(y_test_np, preds, average='weighted')

        # Per-class metrics
        try:
            class_precision, class_recall, class_f1, _ = precision_recall_fscore_support(
                y_test_np, preds, average=None
            )
            female_f1 = class_f1[0] if len(class_f1) > 0 else 0
            male_f1 = class_f1[1] if len(class_f1) > 1 else 0
        except:
            female_f1 = male_f1 = 0

        results.append({
            'threshold': threshold,
            'accuracy': accuracy,
            'f1': f1,
            'precision': precision,
            'recall': recall,
            'female_f1': female_f1,
            'male_f1': male_f1
        })

        #print(f"  Threshold {threshold:.2f}: Acc={accuracy:.4f}, F1={f1:.4f}, Female F1={female_f1:.4f}, Male F1={male_f1:.4f}")

    # Find best threshold (maximize accuracy)
    best_result = max(results, key=lambda x: x['accuracy'])
    best_threshold = best_result['threshold']

    print(f"\nâœ… Optimal probe threshold: {best_threshold:.2f}")
    print(f"  Best Accuracy: {best_result['accuracy']:.4f}")
    print(f"  Best F1: {best_result['f1']:.4f}")
    print(f"  Female F1: {best_result['female_f1']:.4f}")
    print(f"  Male F1: {best_result['male_f1']:.4f}")

    return best_result

"""## MAIN"""

# ====================================================================================
# MAIN EXECUTION - WITH SOTA AND LAMBDA OPTIMIZATION
# ====================================================================================

def main():
    print("ðŸš€ Adversarial Debiasing Pipeline")
    print("="*90)

    # Load data
    csv_path = '/kaggle/working/processed_prof_dataset_with_ict.csv'
    df = load_and_preprocess_data(csv_path)

    # Extract texts and labels from the cleaned dataframe
    texts = df['cleaned_text'].tolist()
    job_labels = df['job_binary'].tolist()
    gender_labels = df['gender_binary'].tolist()

    print(f"\nðŸ“Š Dataset overview:")
    print(f"  Total samples: {len(texts)}")
    print(f"  ICT jobs: {sum(job_labels)} ({sum(job_labels)/len(job_labels)*100:.1f}%)")
    print(f"  Male samples: {sum(gender_labels)} ({sum(gender_labels)/len(gender_labels)*100:.1f}%)")

    # Initialize tokenizer
    tokenizer = AutoTokenizer.from_pretrained("answerdotai/ModernBERT-base")

    # STEP 1: Split into train (70%) + validation (10%) + test (20%)
    print("\nðŸ”„ STEP 1: Creating train/validation/test split")
    print("-"*60)

    # First split: separate test set (20%)
    train_val_texts, test_texts, train_val_job_labels, test_job_labels, train_val_gender_labels, test_gender_labels = train_test_split(
        texts, job_labels, gender_labels, test_size=0.2, random_state=42, stratify=job_labels
    )

    # Second split: separate validation from training (10% of original)
    train_texts, val_texts, train_job_labels, val_job_labels, train_gender_labels, val_gender_labels = train_test_split(
        train_val_texts, train_val_job_labels, train_val_gender_labels, test_size=0.125, random_state=42, stratify=train_val_job_labels
    )

    print(f"Train: {len(train_texts)} samples ({len(train_texts)/len(texts)*100:.1f}%)")
    print(f"Validation: {len(val_texts)} samples ({len(val_texts)/len(texts)*100:.1f}%)")
    print(f"Test: {len(test_texts)} samples ({len(test_texts)/len(texts)*100:.1f}%)")

    # STEP 2: Load best pre-trained model (ICT classifier)
    print("\nðŸ”„ STEP 2: Loading best pre-trained ICT classifier")
    print("-"*60)

    try:
        base_pretrained_model, best_fold_num, best_pretrained_f1 = load_best_pretrained_model(device)
        print(f"âœ… Successfully loaded pre-trained ICT classifier from fold {best_fold_num} with F1: {best_pretrained_f1:.4f}")
    except FileNotFoundError as e:
        print(f"âŒ ERROR: Required pre-trained model not found!")
        print(f"âŒ Please run initial cross-validation first to generate the best model file.")
        raise

    # STEP 3: Run 3-fold cross-validation for baseline performance
    print("\nðŸ”„ STEP 3: Running 3-fold cross-validation for baseline")
    print("-"*60)

    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    fold_results = []
    best_fold = None
    best_overall_f1 = 0.0
    best_model_state = None

    fold_results_detailed = []  # Store detailed results for plotting

    for fold, (train_idx, val_idx) in enumerate(skf.split(np.array(train_texts), np.array(train_job_labels))):
        print(f"\nâ­ FOLD {fold+1}/3 â­")

        # Split data for this fold
        fold_train_texts = [train_texts[i] for i in train_idx]
        fold_train_job_labels = [train_job_labels[i] for i in train_idx]
        fold_train_gender_labels = [train_gender_labels[i] for i in train_idx]

        fold_val_texts = [train_texts[i] for i in val_idx]
        fold_val_job_labels = [train_job_labels[i] for i in val_idx]
        fold_val_gender_labels = [train_gender_labels[i] for i in val_idx]

        # Calculate class weights
        class_weights = calculate_class_weights(fold_train_job_labels, weight_adjustment=1.1)
        class_weights = class_weights.to(device)

        # Create datasets and dataloaders
        train_dataset = ProfessionalDataset(fold_train_texts, fold_train_job_labels, fold_train_gender_labels, tokenizer)
        val_dataset = ProfessionalDataset(fold_val_texts, fold_val_job_labels, fold_val_gender_labels, tokenizer)

        batch_size = 32
        train_loader = create_dataloader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

        print(f"  Fold {fold+1} - Train: {len(train_dataset)}, Val: {len(val_dataset)}")

        # Initialize baseline model for this fold
        config = {
            "hidden_dropout_prob": 0.3,
            "attention_probs_dropout_prob": 0.2
        }

        fold_model = AutoModelForSequenceClassification.from_pretrained(
            "answerdotai/ModernBERT-base",
            num_labels=2,
            low_cpu_mem_usage=True,
            config=config
        ).to(device)

        # Load pre-trained weights
        fold_model.load_state_dict(base_pretrained_model.state_dict())
        baseline_model = BaselineModel(fold_model).to(device)

        # Train baseline model
        results = train_baseline_model_balanced(baseline_model, train_loader, val_loader, device,
                                              class_weights, epochs_frozen=2, epochs_unfrozen=5, accumulation_steps=2)

        fold_results_detailed.append(results)  # Store detailed results for plotting

        # Evaluate and find optimal threshold
        # threshold_result = optimize_threshold(baseline_model, val_loader, device)
        # optimal_threshold = threshold_result['threshold']

        final_metrics = evaluate_baseline_model(baseline_model, val_loader, device,
                                             threshold= 0.5, print_metrics=False) #0.5 instead of optimal_threshold

        overall_f1 = np.mean(final_metrics['f1'])
        ict_f1 = final_metrics['f1'][1]

        fold_result = {
            'fold': fold+1,
            'accuracy': final_metrics['accuracy'],
            'overall_f1': overall_f1,
            'ict_f1': ict_f1,
            #'threshold': optimal_threshold,
            'class_f1': {
                'non_ict': final_metrics['f1'][0],
                'ict': final_metrics['f1'][1]
            }
        }
        fold_results.append(fold_result)

        print(f"  Fold {fold+1} Results: Acc={final_metrics['accuracy']:.4f}, Overall F1={overall_f1:.4f}, ICT F1={ict_f1:.4f}")

        # Track best fold
        if overall_f1 > best_overall_f1:
            best_overall_f1 = overall_f1
            best_fold = fold+1
            best_model_state = baseline_model.bert_classifier.state_dict().copy()
            print(f"  âœ… New best fold!")

    # Print CV summary
    print(f"\nðŸ“‹ 3-FOLD CV SUMMARY:")
    print("-"*50)
    for result in fold_results:
        print(f"Fold {result['fold']}: Acc={result['accuracy']:.4f}, Overall F1={result['overall_f1']:.4f}, ICT F1={result['ict_f1']:.4f}")

    avg_overall_f1 = np.mean([r['overall_f1'] for r in fold_results])
    print(f"Average Overall F1: {avg_overall_f1:.4f}")
    print(f"ðŸ† Best Fold: {best_fold} (Overall F1: {best_overall_f1:.4f})")

    print(f"\nðŸ“Š Plotting CV learning curves for full fine-tuning phase...")
    plot_cv_learning_curves(fold_results_detailed)

    # STEP 4: SOTA Adversary Pre-training (NEW - MOVED HERE)
    print(f"\nðŸ”„ STEP 4: SOTA Adversary Pre-training")
    print("-"*60)

    # Combine train + val for adversary pre-training
    all_train_texts = train_texts + val_texts
    all_train_job_labels = train_job_labels + val_job_labels
    all_train_gender_labels = train_gender_labels + val_gender_labels

    # Calculate class weights for full training data
    final_class_weights = calculate_class_weights(all_train_job_labels, weight_adjustment=1.1)
    final_class_weights = final_class_weights.to(device)

    # Create training dataset
    full_train_dataset = ProfessionalDataset(all_train_texts, all_train_job_labels, all_train_gender_labels, tokenizer)
    full_train_loader = create_dataloader(full_train_dataset, batch_size=32, shuffle=True)

    print(f"Adversary pre-training set: {len(full_train_dataset)} samples")

    # Initialize adversarial model using best CV weights
    config = {
        "hidden_dropout_prob": 0.3,
        "attention_probs_dropout_prob": 0.2
    }
    #add a different config for the final baseline baseline since it's overfitting...
    config_final_baseline = {
        "hidden_dropout_prob": 0.5,
        "attention_probs_dropout_prob": 0.3
    }

    pretrain_adv_base_model = AutoModelForSequenceClassification.from_pretrained(
        "answerdotai/ModernBERT-base",
        num_labels=2,
        low_cpu_mem_usage=True,
        config=config
    ).to(device)

    # Load best CV model weights
    pretrain_adv_base_model.load_state_dict(best_model_state, strict=False)

    # Create adversarial model for pre-training (lambda=1.0 initially)
    pretrain_adversarial_model = AdversarialModel(pretrain_adv_base_model, lambda_=1.0).to(device)

    print(f"ðŸš€ Pre-training adversary to establish strong gender predictor...")

    # Pre-train the adversary
    pretrain_results, pretrain_gender_acc = pretrain_adversary(pretrain_adversarial_model, full_train_loader, device, epochs=10)

    print(f"\nðŸ“Š Plotting adversary pre-training progression...")
    plot_adversary_pretraining(pretrain_results)

    #if pretrain_gender_acc[0] < 0.5:
        #print("âš ï¸ WARNING: Adversary pre-training accuracy low. Consider more epochs.")
    #else:
        #print(f"âœ… Good adversary established (Gender Acc: {pretrain_gender_acc:.4f})")

    # STEP 5: Lambda Parameter Optimization
    print(f"\nðŸ”„ STEP 5: Lambda Parameter Optimization (After Adversary Pre-training)")
    print("-"*60)

    # Create subset for lambda optimization
    lambda_train_size = min(len(all_train_texts), 400)
    lambda_val_size = min(len(val_texts), 100)

    lambda_train_texts = all_train_texts[:lambda_train_size]
    lambda_train_job_labels = all_train_job_labels[:lambda_train_size]
    lambda_train_gender_labels = all_train_gender_labels[:lambda_train_size]

    lambda_val_texts = val_texts[:lambda_val_size]
    lambda_val_job_labels = val_job_labels[:lambda_val_size]
    lambda_val_gender_labels = val_gender_labels[:lambda_val_size]

    # Create datasets for lambda optimization
    lambda_train_dataset = ProfessionalDataset(lambda_train_texts, lambda_train_job_labels,
                                             lambda_train_gender_labels, tokenizer)
    lambda_val_dataset = ProfessionalDataset(lambda_val_texts, lambda_val_job_labels,
                                           lambda_val_gender_labels, tokenizer)

    print(f"Lambda optimization: {len(lambda_train_dataset)} train, {len(lambda_val_dataset)} val samples")

    # Run lambda optimization with pre-trained adversary
    optimal_lambda, lambda_results = optimize_lambda_parameter_with_pretrained_adversary(
        pretrained_model_state=pretrain_adversarial_model.state_dict(),  # Use pre-trained adversary
        train_dataset=lambda_train_dataset,
        val_dataset=lambda_val_dataset,
        device=device,
        class_weights=final_class_weights,
        tokenizer=tokenizer,
        lambda_values=[5.0],# 1.5, 2.0, 5.0],
        max_epochs=6 #from 3 to 6
    )

    # Plot min-max game results
    plot_lambda_optimization_results(lambda_results)
    plot_minmax_game(lambda_results)

    print(f"âœ… Optimal lambda selected: {optimal_lambda}")

    # STEP 6: Final SOTA Training with Optimal Lambda
    print(f"\nðŸ”„ STEP 6: Final SOTA Training with Optimal Lambda={optimal_lambda}")
    print("-"*60)

    # Initialize final adversarial model
    final_adv_base_model = AutoModelForSequenceClassification.from_pretrained(
        "answerdotai/ModernBERT-base",
        num_labels=2,
        low_cpu_mem_usage=True,
        config=config
    ).to(device)

    # Load the pre-trained adversary weights
    final_adversarial_model = AdversarialModel(final_adv_base_model, lambda_=optimal_lambda).to(device)
    final_adversarial_model.load_state_dict(pretrain_adversarial_model.state_dict())

    print(f"ðŸš€ Training final adversarial model with optimal Î»={optimal_lambda}...")
    adversarial_results = train_final_adversarial_model_sota(final_adversarial_model, full_train_loader, device,
                                                            final_class_weights, max_epochs=15, final_lambda=optimal_lambda, schedule_type="linear") #IT WAS 6, I NOW TRY 10!

    print(f"\nðŸ“Š Plotting adversarial training losses...")
    plot_adversarial_losses(adversarial_results)

    # STEP 7: Final evaluation on held-out test set
    print(f"\nðŸ”„ STEP 7: Final evaluation on held-out test set")
    print("="*60)

    # Create baseline model for comparison
    final_base_model = AutoModelForSequenceClassification.from_pretrained(
        "answerdotai/ModernBERT-base",
        num_labels=2,
        low_cpu_mem_usage=True,
        config=config_final_baseline
    ).to(device)

    final_baseline_model = BaselineModel(final_base_model).to(device)
    final_baseline_model.bert_classifier.load_state_dict(best_model_state)


    ##################
    # âœ… ADD THIS DEBUG CODE HERE (after loading weights, before training):
    print(f"ðŸ” Testing loaded model performance...")
    val_dataset_final = ProfessionalDataset(val_texts, val_job_labels, val_gender_labels, tokenizer)
    val_loader_final = DataLoader(val_dataset_final, batch_size=32, shuffle=False)

    test_metrics = evaluate_baseline_model(final_baseline_model, val_loader_final, device, print_metrics=True) #from print_metrics=False
    loaded_f1 = np.mean(test_metrics['f1'])
    print(f"âœ… Loaded model F1: {loaded_f1:.4f} (Expected from CV: {best_overall_f1:.4f})")

    if abs(loaded_f1 - best_overall_f1) > 0.05:
        print(f"âš ï¸ WARNING: Large difference! Something wrong with weight loading.")
    else:
        print(f"âœ… Weight loading successful!")
    # âœ… END DEBUG CODE
    #############################









    # Train final baseline on all data
    print(f"ðŸš€ Training final baseline model on ALL train+val data...")
    baseline_results = train_final_baseline_model(final_baseline_model, full_train_loader, device,
                                                 final_class_weights, max_epochs=10, accumulation_steps=2)

    # Create test dataset
    test_dataset = ProfessionalDataset(test_texts, test_job_labels, test_gender_labels, tokenizer)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

    print(f"Test set: {len(test_dataset)} samples")

    # Find optimal thresholds using validation set (to avoid data leakage)
    # val_dataset_final = ProfessionalDataset(val_texts, val_job_labels, val_gender_labels, tokenizer)
    # val_loader_final = DataLoader(val_dataset_final, batch_size=32, shuffle=False)

    # print(f"\nðŸ” Finding optimal thresholds using validation set...")
    # baseline_threshold_result = optimize_threshold(final_baseline_model, val_loader_final, device, is_adversarial=False)
    # baseline_threshold = baseline_threshold_result['threshold']

    # adversarial_threshold_result = optimize_threshold(final_adversarial_model, val_loader_final, device, is_adversarial=True)
    # adversarial_threshold = adversarial_threshold_result['threshold']

    # print(f"Baseline optimal threshold: {baseline_threshold:.4f}")
    # print(f"Adversarial optimal threshold: {adversarial_threshold:.4f}")

    # Final test set evaluation
    print(f"\nðŸ“Š FINAL TEST SET EVALUATION")
    print("="*60)

    baseline_test_metrics = evaluate_baseline_model(final_baseline_model, test_loader, device,
                                                   threshold=0.50, dataset_name="Test (Baseline)")
    #baseline_gender_acc, _, _, _ = test_gender_predictability(final_baseline_model, test_loader, device)

    adversarial_test_metrics = evaluate_adversarial_model(final_adversarial_model, test_loader, device,
                                                        threshold=0.65, dataset_name="Test (Adversarial)")

    # Test gender predictability from baseline features
    print(f"\nðŸ” Testing gender predictability from baseline features...")

    # Create train+val loaders for probe training
    train_dataset_probe = ProfessionalDataset(all_train_texts, all_train_job_labels, all_train_gender_labels, tokenizer)
    val_dataset_probe = ProfessionalDataset(val_texts, val_job_labels, val_gender_labels, tokenizer)
    train_loader_probe = DataLoader(train_dataset_probe, batch_size=32, shuffle=False)
    val_loader_probe = DataLoader(val_dataset_probe, batch_size=32, shuffle=False)

    baseline_gender_acc, baseline_threshold, baseline_probe_result = test_gender_predictability(final_baseline_model, train_loader_probe, val_loader_probe, test_loader, device)

    # # Display results
    # print(f"\nðŸ“‹ FINAL CORRECTED RESULTS COMPARISON")
    # print("-" * 120)
    # print(f"{'Model':<25} {'Overall F1':<12} {'ICT F1':<10} {'Job Acc':<10} {'Gender Acc':<12} {'Demo Parity':<12}")
    # print("-" * 120)
    # print(f"{'Baseline (Test)':<25} {np.mean(baseline_test_metrics['f1']):<12.4f} {baseline_test_metrics['f1'][1]:<10.4f} {baseline_test_metrics['accuracy']:<10.4f} {baseline_gender_acc:<12.4f} {baseline_test_metrics['demo_parity_diff']:<12.4f}")
    # print(f"{'Adversarial (Test)':<25} {np.mean(adversarial_test_metrics['f1']):<12.4f} {adversarial_test_metrics['f1'][1]:<10.4f} {adversarial_test_metrics['job_accuracy']:<10.4f} {adversarial_test_metrics['gender_accuracy']:<12.4f} {adversarial_test_metrics['demo_parity_diff']:<12.4f}")
    # print("-" * 120)
    # print(f"ðŸ”§ Training Order: ICT Classifier â†’ CV â†’ SOTA Adversary Pre-training â†’ Lambda Optimization â†’ Final SOTA Training")
    # print(f"âš¡ Optimal Lambda: {optimal_lambda}")
    # #print(f"ðŸ“ˆ Adversary Pre-training Accuracy: {pretrain_gender_acc:.4f}")
    # Simple results display - no fancy formatting
    print(f"\nðŸ“‹ FINAL RESULTS")
    print("=" * 50)

    print(f"\nðŸ”¹ final BASELINE MODEL (Test Set):")
    try:
        print(f"  Job Accuracy: {baseline_test_metrics.get('accuracy', 'N/A')}")
        print(f"  Overall F1: {np.mean(baseline_test_metrics.get('f1', [0, 0])):.4f}")
        #print(f"  ICT F1: {baseline_test_metrics.get('f1', [0, 0])[1] if baseline_test_metrics.get('f1') is not None else 'N/A'}")
        print(f"  Gender Predictability: {baseline_gender_acc:.4f}")
        print(f"  Demographic Parity Diff: {baseline_test_metrics.get('demo_parity_diff', 'N/A')}")
        print("probe gender accuracy:", best_result['accuracy'])
    except Exception as e:
        print(f"  Error displaying baseline metrics: {e}")

    print(f"\nðŸ”¹ ADVERSARIAL MODEL (Test Set):")
    try:
        print(f"  Job Accuracy: {adversarial_test_metrics.get('job_accuracy', 'N/A')}")
        print(f"  Overall F1: {np.mean(adversarial_test_metrics.get('f1', [0, 0])):.4f}")
        print(f"  ICT F1: {adversarial_test_metrics.get('f1', [0, 0])[1] if adversarial_test_metrics.get('f1') is not None else 'N/A'}")
        print(f"  Gender Predictability: {adversarial_test_metrics.get('gender_accuracy', 'N/A')}")
        print(f"  Demographic Parity Diff: {adversarial_test_metrics.get('demo_parity_diff', 'N/A')}")
    except Exception as e:
        print(f"  Error displaying adversarial metrics: {e}")

    print(f"\nðŸ”¹ TRAINING INFO:")
    print(f"  Optimal Lambda: {optimal_lambda}")
    print(f"  Training Order: ICT â†’ CV â†’ Adversary Pre-training â†’ Lambda Opt â†’ Final SOTA")

    print(f"\nðŸ”¹ BIAS REDUCTION:")
    try:
        baseline_gender = baseline_gender_acc if baseline_gender_acc is not None else 0
        adversarial_gender = adversarial_test_metrics.get('gender_accuracy', 0)
        if adversarial_gender is not None:
            bias_reduction = baseline_gender - adversarial_gender
            print(f"  Gender Predictability: {baseline_gender:.4f} â†’ {adversarial_gender:.4f}")
            print(f"  Bias Reduction: {bias_reduction:.4f} ({bias_reduction/baseline_gender*100:.1f}% decrease)")
        else:
            print(f"  Bias Reduction: Could not calculate")
    except Exception as e:
        print(f"  Error calculating bias reduction: {e}")

    # Success criteria
    print(f"\nâœ… SUCCESS CRITERIA CHECK:")
    job_f1_drop = np.mean(baseline_test_metrics.get('f1', [0])) - np.mean(adversarial_test_metrics.get('f1', [0]))
    #baseline_gender = baseline_gender_acc if baseline_gender_acc is not None else 0.5
    #gender_acc_drop = baseline_gender - adversarial_test_metrics.get('gender_accuracy', 0.5)


    #print(f"1. Job F1 drop < 0.1: {job_f1_drop:.4f} {'âœ…' if job_f1_drop < 0.1 else 'we could improve this'}")
    #print(f"2. Gender Acc drop > 0.20: {gender_acc_drop:.4f} {'âœ…' if gender_acc_drop > 0.20 else 'âŒ'}")
    #print(f"3. Adversarial Gender Acc < 0.60: {adversarial_test_metrics['gender_accuracy']:.4f} {'âœ…' if adversarial_test_metrics['gender_accuracy'] < 0.60 else 'we could improve this'}")
    #print(f"4. Adversary Pre-training: {pretrain_gender_acc:.4f}{'âœ…' if pretrain_gender_acc > 0.60 else ''}")

    return {
        'cv_results': fold_results,
        'best_fold': best_fold,
        'pretrain_gender_acc': pretrain_gender_acc,
        'optimal_lambda': optimal_lambda,
        'lambda_results': lambda_results,
        'test_baseline': baseline_test_metrics,
        'test_adversarial': adversarial_test_metrics,
        'baseline_gender_acc': baseline_gender_acc,
        'adversarial_training_history': adversarial_results,
        'training_order': 'ICT â†’ CV â†’ SOTA Pre-training â†’ Lambda Opt â†’ Final SOTA'
    }

# ====================================================================================
# HELPER FUNCTIONS FOR CORRECTED PIPELINE
# ====================================================================================

def test_gender_predictability(model, train_loader, val_loader, test_loader, device):
    """Test how well gender can be predicted from baseline model features"""
    model.eval()
    train_features = []
    train_gender_labels = []
    test_features = []
    test_gender_labels = []
    print(f"\nðŸ” Extracting features for proper train/test split:")

    with torch.no_grad():
        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            gender_labels = batch['gender_labels']

            outputs = model.bert_classifier.model(input_ids=input_ids, attention_mask=attention_mask)
            pooled_output = outputs.last_hidden_state[:, 0, :]

            train_features.append(pooled_output.cpu())
            train_gender_labels.extend(gender_labels.numpy())

        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            gender_labels = batch['gender_labels']

            outputs = model.bert_classifier.model(input_ids=input_ids, attention_mask=attention_mask)
            pooled_output = outputs.last_hidden_state[:, 0, :]

            train_features.append(pooled_output.cpu())
            train_gender_labels.extend(gender_labels.numpy())

        # Extract test features for probe testing
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            gender_labels = batch['gender_labels']

            outputs = model.bert_classifier.model(input_ids=input_ids, attention_mask=attention_mask)
            pooled_output = outputs.last_hidden_state[:, 0, :]

            test_features.append(pooled_output.cpu())
            test_gender_labels.extend(gender_labels.numpy())

    # Prepare training data (from train+val)
    X_train_probe = torch.cat(train_features, dim=0)
    y_train_probe = torch.tensor(train_gender_labels, dtype=torch.long)

    # Prepare testing data (from test)
    X_test_probe = torch.cat(test_features, dim=0)
    y_test_probe = torch.tensor(test_gender_labels, dtype=torch.long)

    print(f"\nðŸ” Gender Probe Training:")
    print(f"  Probe train: {len(X_train_probe)} samples")
    print(f"  Probe test: {len(X_test_probe)} samples")

    # Much simpler architecture for small dataset
    gender_classifier = nn.Sequential(
        nn.Linear(X_train_probe.shape[1], 64),  # 768 â†’ 64 (much smaller)
        nn.ReLU(),
        nn.Dropout(0.5),  # Higher dropout for regularization
        nn.Linear(64, 2)
    )
    gender_optimizer = torch.optim.Adam(gender_classifier.parameters(), lr=1e-4)


    gender_criterion = nn.CrossEntropyLoss()
    #DEBUG

    #gender_dataset = torch.utils.data.TensorDataset(all_features, torch.tensor(all_gender_labels, dtype=torch.long))
    #gender_loader = DataLoader(gender_dataset, batch_size=32, shuffle=True)

    # new, Create data loaders
    train_dataset = torch.utils.data.TensorDataset(X_train_probe, y_train_probe)
    test_dataset = torch.utils.data.TensorDataset(X_test_probe, y_test_probe)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_loader_probe = DataLoader(test_dataset, batch_size=32, shuffle=False)

    #for debugging
    print(f"\nðŸ” Probe Data Balance Check:")
    print(f"  Train: Female={torch.sum(y_train_probe == 0).item()}, Male={torch.sum(y_train_probe == 1).item()}")
    print(f"  Test: Female={torch.sum(y_test_probe == 0).item()}, Male={torch.sum(y_test_probe == 1).item()}")
    print(f"  Train Female %: {torch.sum(y_train_probe == 0).item() / len(y_train_probe) * 100:.1f}%")
    print(f"  Test Female %: {torch.sum(y_test_probe == 0).item() / len(y_test_probe) * 100:.1f}%")

    # Initialize metrics storage
    train_losses = []
    train_accs = []
    test_accs = []
    test_f1s = []

    # Early stopping to prevent overfitting
    best_test_acc = 0
    patience = 5
    patience_counter = 0
    best_model_state = None

    print(f"\nðŸ“ˆ Training Progress:")
    #print(f"{'Epoch':<6} {'Loss':<8} {'Train Acc':<10} {'Test Acc':<10} {'Test F1':<8}")
    print("-" * 50)


    #gender_classifier.train()
    for epoch in range(15):
        # Training
        gender_classifier.train()
        epoch_loss = 0.0


        for features, labels in train_loader:
            gender_optimizer.zero_grad()
            logits = gender_classifier(features)
            loss = gender_criterion(logits, labels)
            loss.backward()
            gender_optimizer.step()
            epoch_loss += loss.item()

        # Evaluation
        gender_classifier.eval()

        with torch.no_grad():
            # Train accuracy
            train_logits = gender_classifier(X_train_probe)
            train_preds = torch.argmax(train_logits, dim=1)
            train_acc = (train_preds == y_train_probe).float().mean().item()

            # Test accuracy and F1
            test_logits = gender_classifier(X_test_probe)
            test_preds = torch.argmax(test_logits, dim=1)
            test_acc = (test_preds == y_test_probe).float().mean().item()

            # F1 score
            test_f1 = f1_score(y_test_probe.cpu().numpy(), test_preds.cpu().numpy(), average='weighted')

        # Store metrics
        avg_loss = epoch_loss / len(train_loader)
        train_losses.append(avg_loss)
        train_accs.append(train_acc)
        test_accs.append(test_acc)
        test_f1s.append(test_f1)

                # Early stopping check
        if test_acc > best_test_acc:
            best_test_acc = test_acc
            patience_counter = 0
            best_model_state = gender_classifier.state_dict().copy()
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch+1}")
                break

        # Print immediately
        print(f"Epoch {epoch+1:2d}/10: Loss={avg_loss:.4f} | Train Acc={train_acc:.4f} | Test Acc={test_acc:.4f} | Test F1={test_f1:.4f}")

    print(f"\nâœ… Training completed! Collected {len(train_losses)} epochs of data")

    # Threshold optimization for probe
    probe_threshold_result = optimize_probe_threshold(gender_classifier, X_test_probe, y_test_probe, device)
    optimal_probe_threshold = probe_threshold_result['threshold']

    # Apply optimal threshold for final evaluation
    gender_classifier.eval()
    with torch.no_grad():
        final_logits = gender_classifier(X_test_probe)
        final_probs = F.softmax(final_logits, dim=1)[:, 1]
        optimized_preds = (final_probs > optimal_probe_threshold).long()
        optimized_accuracy = (optimized_preds == y_test_probe).float().mean().item()

    print(f"\nðŸ“Š Final Probe Results:")
    print(f"  Default threshold (0.5): {test_acc:.4f}")
    print(f"  Optimized threshold ({optimal_probe_threshold:.2f}): {optimized_accuracy:.4f}")
    print(f"  Improvement: {optimized_accuracy - test_acc:.4f}")

    # Update final_preds for confusion matrix
    final_preds = optimized_preds.cpu().numpy()

    # Debug check
    print(f"\nDEBUG - Final metrics lengths:")
    print(f"  train_losses: {len(train_losses)}")
    print(f"  train_accs: {len(train_accs)}")
    print(f"  test_accs: {len(test_accs)}")
    print(f"  test_f1s: {len(test_f1s)}")




    # Simple plotting with safety checks
    if len(train_losses) >= 2:  # Need at least 2 points to plot
        import matplotlib.pyplot as plt

        epochs_range = list(range(1, len(train_losses) + 1))

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # Loss plot
        ax1.plot(epochs_range, train_losses, 'b-o', linewidth=2, markersize=6)
        ax1.set_title(f'Training Loss ({len(train_losses)} epochs)', fontsize=14)
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.grid(True, alpha=0.3)

        # Accuracy comparison
        ax2.plot(epochs_range, train_accs, 'b-o', label='Train Acc', linewidth=2, markersize=6)
        ax2.plot(epochs_range, test_accs, 'r-o', label='Test Acc', linewidth=2, markersize=6)
        ax2.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='Random (50%)')
        ax2.set_title(f'Accuracy Progression ({len(train_losses)} epochs)', fontsize=14)
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # F1 Score
        ax3.plot(epochs_range, test_f1s, 'g-o', linewidth=2, markersize=6)
        ax3.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='Random baseline')
        ax3.set_title(f'Test F1 Score ({len(train_losses)} epochs)', fontsize=14)
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('F1 Score')
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # Confusion Matrix (final epoch)
        from sklearn.metrics import confusion_matrix
        import seaborn as sns

        gender_classifier.eval()
        with torch.no_grad():
            final_logits = gender_classifier(X_test_probe)
            final_preds = torch.argmax(final_logits, dim=1).cpu().numpy()

        cm = confusion_matrix(y_test_probe.cpu().numpy(), final_preds)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=['Female', 'Male'], yticklabels=['Female', 'Male'],
                    ax=ax4)
        ax4.set_title('Final Confusion Matrix', fontsize=14)
        ax4.set_xlabel('Predicted')
        ax4.set_ylabel('Actual')

        plt.tight_layout()
        plt.show()
    else:
        print(f"âš ï¸ Not enough data to plot (only {len(train_losses)} epochs)")

        # DEBUG:
    print(f"\nðŸ” Debug probe results:")
    print(f"  Final test_acc type: {type(test_acc)}")
    print(f"  Final test_acc value: {test_acc}")
    print(f"  Returning: {test_acc}")

    return optimized_accuracy, optimal_probe_threshold, probe_threshold_result


####################################

def plot_minmax_game(lambda_results):
    """Plot the min-max game between job F1 and gender accuracy"""
    lambdas = [r['lambda'] for r in lambda_results]
    job_f1s = [r['job_f1'] for r in lambda_results]
    gender_accs = [r['gender_accuracy'] for r in lambda_results]

    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))

    # Plot 1: Job F1 vs Lambda
    ax1.plot(lambdas, job_f1s, 'b-o', linewidth=2, markersize=8, label='Job F1')
    ax1.set_xlabel('Lambda (Î»)')
    ax1.set_ylabel('Job F1 Score')
    ax1.set_title('Job Performance vs Lambda\n(Higher is Better)')
    ax1.grid(True, alpha=0.3)
    ax1.legend()

    # Plot 2: Gender Accuracy vs Lambda
    ax2.plot(lambdas, gender_accs, 'r-o', linewidth=2, markersize=8, label='Gender Accuracy')
    ax2.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='Random (50%)')
    ax2.set_xlabel('Lambda (Î»)')
    ax2.set_ylabel('Gender Prediction Accuracy')
    ax2.set_title('Gender Predictability vs Lambda\n(Lower is Better for Debiasing)')
    ax2.grid(True, alpha=0.3)
    ax2.legend()

    # Plot 3: Trade-off (Job F1 vs Gender Acc)
    ax3.scatter(gender_accs, job_f1s, c=lambdas, cmap='viridis', s=100, alpha=0.7)
    for i, lambda_val in enumerate(lambdas):
        ax3.annotate(f'Î»={lambda_val}', (gender_accs[i], job_f1s[i]),
                    xytext=(5, 5), textcoords='offset points', fontsize=9)

    ax3.axvline(x=0.5, color='gray', linestyle='--', alpha=0.7, label='Random Gender Prediction')
    ax3.set_xlabel('Gender Prediction Accuracy (Lower = More Debiased)')
    ax3.set_ylabel('Job F1 Score (Higher = Better Performance)')
    ax3.set_title('Min-Max Game: Job Performance vs Debiasing\n(Bottom-Right is Ideal)')
    ax3.grid(True, alpha=0.3)
    ax3.legend()

    # Add colorbar for lambda values
    cbar = plt.colorbar(ax3.collections[0], ax=ax3)
    cbar.set_label('Lambda (Î»)')

    plt.tight_layout()
    plt.show()

# Run the corrected main function
if __name__ == "__main__":
    results = main()

